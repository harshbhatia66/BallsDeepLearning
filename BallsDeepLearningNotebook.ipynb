{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Modeling Problem\n",
    "\n",
    "In this project, we delve into the exciting world of Natural Language Processing (NLP) to tackle a compelling challenge: developing a chatbot that not only engages in contextually relevant conversations but also emulates the unique conversational styles of characters from the popular TV show \"Rick and Morty\". We chose to work with the DialoGPT model from Microsoft - a variant of the GPT-2 model, that is fine-tuned specifically for dialogues. \n",
    "\n",
    "Our project involves distinct challenges, each representing a key stage in the development of our chatbot:\n",
    "\n",
    "1. **Data Acquisition and Cleaning**: Our first task was to obtain and clean a dataset with enough depth and richness to effectively train our model. This required sourcing and preprocessing dialogues from the TV show.\n",
    "\n",
    "2. **Understanding Character Styles**: Each character in \"Rick and Morty\" boasts a unique conversational style, peppered with their own idiosyncrasies. The complexity of capturing the nuances of their dialogue, including humor, colloquialisms, and catchphrases, represented a significant challenge.\n",
    "\n",
    "3. **Text Generation**: Beyond generating relevant responses, our model needed to convincingly replicate the distinct conversational style of the character in question. The GPT-2, or more precisely, its DialogPT variant was our tool of choice, a finetuned model in generating conversational responses.\n",
    "\n",
    "4. **Evaluation**: Our final hurdle lay in evaluating our model's performance. Apart from traditional accuracy metrics, we had to assess whether the generated text genuinely encapsulated the character's style. Establishing evaluation metrics represented a unique and interesting problem in itself.\n",
    "\n",
    "___\n",
    "\n",
    "## Goal of Our Project\n",
    "\n",
    "The primary aim of our project is to leverage Natural Language Processing (NLP) techniques to develop an interactive and unique artificial personality. Our focus is not only to ensure contextually relevant conversations but also to emulate the distinctive conversational styles of characters from the TV show \"Rick and Morty\". By utilizing the DialoGPT model, a fine-tuned variant of the GPT-2 model specialized for dialogues, we seek to craft a chatbot that engages users in fun interactions, bringing the personas of \"Rick and Morty\" into our NLP model. Through this project we aim to expand our knowledge of conversational AI models and testing the boundaries.\n",
    "\n",
    "___\n",
    "\n",
    "## Extending NLP-Powered Chatbot to Real Business Solutions and Beyond\n",
    "\n",
    "The application of our NLP-powered chatbot could potentially extend beyond just creating a fun character from one popular show \"Rick and Morty\". With a successful Artifical Persona we can create unique and interactive user experiences on various platforms, presenting exciting opportunities for businesses and content creators.\n",
    "\n",
    "1. **Website with Cool Characters:** We could create a business that can leverage our NLP Artifical Persona Chatbot to design engaging websites featuring interactive and entertaining characters. These characters could be original creations or inspired by popular media franchises. \n",
    "\n",
    "2. **Interactive YouTubers:** Content creators, especially YouTubers, can use our NLP-powered Artifical Persona to bring their personas to life in a whole new way. Imagine a YouTube channel where the creator's virtual character interacts with viewers in real-time, responding to comments and engaging in witty banter. If the Artifical Persona is indistinguishable from the real person this personalized and interactive experience could foster a stronger sense of connection between the content creator and their audience, enhancing viewer experience.\n",
    "\n",
    "3. **Customizable Chatbots for Customer Support:** Beyond entertainment, our chatbot technology can serve as a valuable tool for businesses to streamline customer support services. Companies can create customized chatbots with distinct personalities that align with their brand identity. These chatbots can efficiently handle customer inquiries, provide helpful information, and resolve issues, all while maintaining a friendly and engaging conversational style.\n",
    "\n",
    "4. **Educational Applications:** In the realm of education, our NLP-powered chatbot can be utilized to create interactive and adaptive learning experiences. Students can engage with virtual characters that act as intelligent tutors, providing personalized explanations, quizzes, and feedback based on individual learning preferences.\n",
    "\n",
    "5. **Virtual Influencers:** As the world of social media continues to evolve, virtual influencers have gained popularity. Our chatbot technology can enable the creation of virtual influencers with unique personalities, who can interact with followers on social media platforms. These virtual influencers can promote products, share content, and engage in conversations with users, offering a fresh approach to influencer marketing.\n",
    "\n",
    "The potential applications of our NLP-powered Artifical Persona chatbot are vast and diverse. From enhancing user experiences on websites to transforming the way content creators interact with their audience, and from enhancing customer support to improving education and influencer marketing, the possibilities are limitless. Through this project, we are exploring the capabilities of NLPs and how a successful model could integrate with todays world.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Model\n",
    "\n",
    "In this project, we decided to utilize Microsoft's DialoGPT model. As a variation of the GPT-2 model fine-tuned explicitly for generating conversational responses, DialoGPT seemed well-suited for our task of creating a chatbot that mirrors the dialogue style in the \"Rick and Morty\" show. This model selection was also backed by the promising results demonstrated by DialoGPT in generating contextually relevant and human-like conversational responses.\n",
    "\n",
    "In the realm of Natural Language Processing (NLP), multiple models could have been considered. For instance, sequence-to-sequence (Seq2Seq) models like LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Units) networks could have been potential choices. However, these recurrent neural network-based models often struggle with long sequences due to the vanishing gradient problem, making them less ideal for dialog systems where the context could be quite lengthy.\n",
    "\n",
    "On the other hand, Transformer-based models like GPT-2 overcome this limitation with their attention mechanism. Although the GPT-2 model is already quite powerful, DialoGPT takes it a step further by fine-tuning this base model on a large-scale dialogue dataset. This fine-tuning makes DialoGPT more apt at understanding and generating dialogue, a feature that aligns perfectly with our project goal.\n",
    "\n",
    "Our aim was to mimic the humoristic style of \"Rick and Morty\" accurately with a focus on the character of Rick, and also generate contextually coherent and relevant responses in a conversational setting. The DialoGPT model provided a solid foundation for this, allowing us to further train and fine-tune it with our dataset for even better performance.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Deep Dive\n",
    "\n",
    "In this section, we delve into our dataset, which is the cornerstone of any Machine Learning project. We'll explore the data acquisition process, our method for preparing the data for our model, and some challenges associated with the nature of the data and our approach. \n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "Our dataset comes from Kaggle, specifically the [Rick and Morty Scripts dataset](https://www.kaggle.com/andradaolteanu/rickmorty-scripts) posted by user Andrada Olteanu. This dataset provides lines of dialogue from different characters across various episodes of the show.\n",
    "\n",
    "The dataset structure is as follows:\n",
    "\n",
    "- **index**: A simple row identifier.\n",
    "- **season no.**: The season in which the dialogue line appears.\n",
    "- **episode no.**: The episode in which the dialogue line appears.\n",
    "- **episode name**: The name of the episode.\n",
    "- **name**: The character who speaks the line.\n",
    "- **line**: The line of dialogue itself.\n",
    "\n",
    "Here are the first few lines for context:\n",
    "\n",
    "```\n",
    "index\tseason no.\tepisode no.\tepisode name\tname\tline\n",
    "0\t1\t1\tPilot\tRick\tMorty! You gotta come on. Jus'... you gotta come with me.\n",
    "1\t1\t1\tPilot\tMorty\tWhat, Rick? Whatâ€™s going on?\n",
    "2\t1\t1\tPilot\tRick\tI got a surprise for you, Morty.\n",
    "```\n",
    "___\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "One of our major tasks in data preparation is formatting this dataset for training our model, which requires a context-response structure. Each response will have the `n` previous responses as its context. We decided that `n = 7`, meaning each response has the seven preceding lines of dialogue as its context. The reason for this decision is that, in most conversations, the last 7 exchanges provide a sufficient context for understanding the current response.\n",
    "\n",
    "Here's where you might choose to include a code snippet demonstrating how you generate the context for each line. (This is merely a suggestion, depending on your workflow and your professor's expectations).\n",
    "\n",
    "Once our dataset is structured appropriately, we divide it into training and test subsets. This split allows us to train our model on one set of data (training) and evaluate its performance on a separate set (test) that it has not seen before.\n",
    "\n",
    "Finally, we convert our dataset into a format suitable for our DialogPT model. This requires concatenating the responses into a single string for each row, separated by a special 'end of string' token. This token enables the model to identify the end of each response in a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T05:00:52.684521Z",
     "iopub.status.busy": "2023-08-01T05:00:52.683773Z",
     "iopub.status.idle": "2023-08-01T05:00:52.770554Z",
     "shell.execute_reply": "2023-08-01T05:00:52.769965Z",
     "shell.execute_reply.started": "2023-08-01T05:00:52.684490Z"
    },
    "id": "6QzjJNm_OosG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "      <td>What, Rick? Whatâ€™s going on?</td>\n",
       "      <td>Morty! You gotta come on. Jus'... you gotta co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "      <td>What, Rick? Whatâ€™s going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morty. I had to... I had to do it. I hadâ€” I ha...</td>\n",
       "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What?! A bomb?!</td>\n",
       "      <td>Morty. I had to... I had to do it. I hadâ€” I ha...</td>\n",
       "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're gonna drop it down there just get a whol...</td>\n",
       "      <td>What?! A bomb?!</td>\n",
       "      <td>Morty. I had to... I had to do it. I hadâ€” I ha...</td>\n",
       "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  What do you think of this... flying vehicle, M...   \n",
       "1  Yeah, Rick... I-it's great. Is this the surprise?   \n",
       "2  Morty. I had to... I had to do it. I hadâ€” I ha...   \n",
       "3                                    What?! A bomb?!   \n",
       "4  We're gonna drop it down there just get a whol...   \n",
       "\n",
       "                                             context  \\\n",
       "0  We gotta go, gotta get outta here, come on. Go...   \n",
       "1  What do you think of this... flying vehicle, M...   \n",
       "2  Yeah, Rick... I-it's great. Is this the surprise?   \n",
       "3  Morty. I had to... I had to do it. I hadâ€” I ha...   \n",
       "4                                    What?! A bomb?!   \n",
       "\n",
       "                                           context/0  \\\n",
       "0                Ow! Ow! You're tugging me too hard!   \n",
       "1  We gotta go, gotta get outta here, come on. Go...   \n",
       "2  What do you think of this... flying vehicle, M...   \n",
       "3  Yeah, Rick... I-it's great. Is this the surprise?   \n",
       "4  Morty. I had to... I had to do it. I hadâ€” I ha...   \n",
       "\n",
       "                                           context/1  \\\n",
       "0  Come on, I got a surprise for you.  Come on, h...   \n",
       "1                Ow! Ow! You're tugging me too hard!   \n",
       "2  We gotta go, gotta get outta here, come on. Go...   \n",
       "3  What do you think of this... flying vehicle, M...   \n",
       "4  Yeah, Rick... I-it's great. Is this the surprise?   \n",
       "\n",
       "                                           context/2  \\\n",
       "0  It's the middle of the night. What are you tal...   \n",
       "1  Come on, I got a surprise for you.  Come on, h...   \n",
       "2                Ow! Ow! You're tugging me too hard!   \n",
       "3  We gotta go, gotta get outta here, come on. Go...   \n",
       "4  What do you think of this... flying vehicle, M...   \n",
       "\n",
       "                                           context/3  \\\n",
       "0                   I got a surprise for you, Morty.   \n",
       "1  It's the middle of the night. What are you tal...   \n",
       "2  Come on, I got a surprise for you.  Come on, h...   \n",
       "3                Ow! Ow! You're tugging me too hard!   \n",
       "4  We gotta go, gotta get outta here, come on. Go...   \n",
       "\n",
       "                                           context/4  \\\n",
       "0                       What, Rick? Whatâ€™s going on?   \n",
       "1                   I got a surprise for you, Morty.   \n",
       "2  It's the middle of the night. What are you tal...   \n",
       "3  Come on, I got a surprise for you.  Come on, h...   \n",
       "4                Ow! Ow! You're tugging me too hard!   \n",
       "\n",
       "                                           context/5  \n",
       "0  Morty! You gotta come on. Jus'... you gotta co...  \n",
       "1                       What, Rick? Whatâ€™s going on?  \n",
       "2                   I got a surprise for you, Morty.  \n",
       "3  It's the middle of the night. What are you tal...  \n",
       "4  Come on, I got a surprise for you.  Come on, h...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Args()\n",
    "# Read dataset in the same directory\n",
    "all_rick = pd.read_csv('RickAndMortyScripts.csv')\n",
    "contexted = []\n",
    "# Number of previous responses\n",
    "n = 7\n",
    "\n",
    "# Create context-response pairs, previous n lines are used as context for every lines of dialogue\n",
    "for i in range(n, len(all_rick['line'])):\n",
    "  row = []\n",
    "  prev = i - 1 - n\n",
    "  for j in range(i, prev, -1):\n",
    "    row.append(all_rick['line'][j])\n",
    "  contexted.append(row)\n",
    "\n",
    "columns = ['response', 'context']\n",
    "columns = columns + ['context/'+ str(i) for i in range(n - 1)]\n",
    "\n",
    "# Convert contexted list into a pandas dataframe, each row corresponds to a context-response pair and column represent the response and the different lines of context\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "# Splitting data into 9:1 so that 90% is used for training and remaining for validation\n",
    "trn_df, val_df = train_test_split(df, test_size = 0.1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we encode our text data into a sequence of integers, each representing a token, which is then flattened into a single list for the model to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.103528Z",
     "iopub.status.busy": "2023-07-31T09:10:05.103264Z",
     "iopub.status.idle": "2023-07-31T09:10:05.107812Z",
     "shell.execute_reply": "2023-07-31T09:10:05.106998Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.103466Z"
    },
    "id": "ihxFi5q4Omjt"
   },
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    \"\"\"\n",
    "    Function to encode text data into a format that the model can understand.\n",
    "    \"\"\"\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    # Encodes each piece of text into a sequence of integers, each integer represents a token\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    # Flatten the list of encoded sequences into a single list\n",
    "    conv = flatten(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our custom ConversationDataset class, which inherits from PyTorch's Dataset class. It handles the preparation of conversational data for model training or evaluation, caching the processed data for efficiency, and returning each item as a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.109306Z",
     "iopub.status.busy": "2023-07-31T09:10:05.109141Z",
     "iopub.status.idle": "2023-07-31T09:10:05.115665Z",
     "shell.execute_reply": "2023-07-31T09:10:05.114660Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.109289Z"
    },
    "id": "0JQcjeQZOj5M"
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Defines a custom class, which inherits from the Dataset class of Pytorch.\n",
    "    This will handle the prepraration of conversational data for model training or evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "        # Constructor of the class\n",
    "        # Adjusts the block size based on the maximum length that the model and tokenizer can handle\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        # Cached the processed data\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        # Check if cached file already exists\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Returns the item as a Pytorch tensor\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflecting on the Data\n",
    "\n",
    "There are a few key considerations worth discussing about our dataset and the way we have chosen to use it:\n",
    "\n",
    "1. **Limited Data**: Our dataset consists of only about 2,500 lines of dialogue. This is quite a small dataset for a machine learning project. Because the GPT-2 model was initially trained on a much larger dataset, fine-tuning it on our smaller dataset may lead to overfitting, where the model performs well on the training data but fails to generalize to unseen data.\n",
    "\n",
    "2. **Character Diversity**: Each line in the dataset can come from a different character, adding another level of complexity to the task. The model needs to learn not only the response to a given line but also which character is likely to say that response. Given the limited size of our dataset, this could affect the model's ability to accurately capture each character's style.\n",
    "\n",
    "3. **Context Length**: The choice of using seven previous responses as context is somewhat arbitrary and could affect the model's performance. If the context is too short, the model may not have enough information to generate an appropriate response. On the other hand, if the context is too long, the model may become too complex and harder to train.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Let's turn our attention to the initial step of any data science project: Exploratory Data Analysis (EDA).\n",
    "\n",
    "EDA is a critical step that allows us to understand the data we're working with. By visualizing, summarizing, and interpreting the data, we can gain valuable insights that guide our model selection and training process. It also helps us identify any potential issues with the data, such as missing values or outliers, that could impact the performance of our models.\n",
    "\n",
    "\n",
    "### Unigrams, Bigrams, and Trigrams\n",
    "\n",
    "In the field of Natural Language Processing (NLP), unigrams, bigrams, and trigrams are essential components of text analysis. They represent sequences of one, two, and three words respectively. For example, given the text \"come on Morty\", the unigrams are \"come\", \"on\", \"Morty\"; the bigrams are \"come on\", \"on Morty\"; and the trigram is \"come on Morty\".\n",
    "\n",
    "One common application of unigrams, bigrams, and trigrams is in the creation of a Term Document Matrix (TDM). A TDM is a mathematical matrix that describes the frequency of terms (unigrams, bigrams, trigrams, etc.) that occur in a collection of documents. In a TDM, rows represent the terms found in the documents, columns represent the documents, and the values represent the frequency of each term in each document.\n",
    "\n",
    "Below is the Python code that defines functions to generate unigrams, bigrams, and trigrams from a given text, and to create a TDM from these n-grams.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to create a term-document matrix of n-grams\n",
    "def create_tdm(text, ngram_range):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    tdm = vectorizer.fit_transform(text)\n",
    "    return pd.DataFrame(tdm.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Function to create unigrams\n",
    "def create_unigrams(text):\n",
    "    return create_tdm(text, (1, 1))\n",
    "\n",
    "# Function to create bigrams\n",
    "def create_bigrams(text):\n",
    "    return create_tdm(text, (2, 2))\n",
    "\n",
    "# Function to create trigrams\n",
    "def create_trigrams(text):\n",
    "    return create_tdm(text, (3, 3))\n",
    "```\n",
    "\n",
    "These functions helped to analyze the frequency of unigrams, bigrams, and trigrams in our text data, which can provide insights into the most common words and phrases, the relationships between words, and the overall structure of the text.\n",
    "\n",
    "<img src=\"assets/Morty_unigram_frequency_chart.png\" alt=\"1\" height=\"250\">\n",
    "<img src=\"assets/Morty_bigram_frequency_chart.png\" alt=\"2\" height=\"250\">\n",
    "<img src=\"assets/Morty_trigram_frequency_chart.png\" alt=\"3\" height=\"250\">\n",
    "<img src=\"assets/Rick_unigram_frequency_chart.png\" alt=\"4\" height=\"250\">\n",
    "<img src=\"assets/Rick_bigram_frequency_chart.png\" alt=\"5\" height=\"250\">\n",
    "<img src=\"assets/Rick_trigram_frequency_chart.png\" alt=\"6\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue Analysis: Who Talks the Most?\n",
    "\n",
    "In any dialogue-based dataset, one interesting aspect to explore is the distribution of dialogues among the characters. This can provide insights into the dynamics of the character interactions and the prominence of each character in the dataset.\n",
    "\n",
    "In the case of our Rick and Morty dataset, it's not surprising to find that Rick and Morty, being the titular characters, have the most dialogues. Following them are Beth, Jerry, and Summer, completing the Smith family.\n",
    "\n",
    "Interestingly, we've chosen to treat 'Pickle Rick' as a character on his own, separate from 'Rick'. This decision was made due to the distinct personality that Pickle Rick exhibits in the show, which could be considered somewhat more 'sour' than the 'true' Rick.\n",
    "\n",
    "Below we have provided graphs to help us understand the character dialogue frequency\n",
    "\n",
    "<img src=\"assets/character_line_frequencies_chart.png\" alt=\"7\" height=\"300\">\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Shape and Its Use in DialoGPT (A GPT-2 Variant)\n",
    "\n",
    "The input data to GPT-2 (and in our case, the DialogPT variant we're using) is structured as a 2D tensor, where the first dimension represents individual instances (in our case, dialogues) and the second dimension represents the tokenized words within each instance.\n",
    "\n",
    "This is because, at its core, GPT-2 is a transformer model, which was designed to handle sequence data, and text data is fundamentally a sequence of words or tokens. By representing our text data as a 2D tensor, we can maintain the sequence nature of our data while enabling the model to process multiple instances simultaneously for efficient batch processing.\n",
    "\n",
    "To better understand the shape of our input data, consider the following example:\n",
    "\n",
    "```python\n",
    "inputs = [[\"I\", \"am\", \"GPT-2\"], [\"Hello\", \"world\"]]\n",
    "```\n",
    "\n",
    "This list of lists represents two sentences, each of which is a sequence of words. After tokenization and numerical encoding, our data might look something like this:\n",
    "\n",
    "```python\n",
    "inputs = [[9, 84, 30522], [15496, 2327]]\n",
    "```\n",
    "\n",
    "However, because tensor operations require our data to be in a regular shape (i.e., each instance must have the same length), we need to pad our data to account for sentences of different lengths:\n",
    "\n",
    "```python\n",
    "inputs = [[9, 84, 30522, 0], [15496, 2327, 0, 0]]\n",
    "```\n",
    "\n",
    "Now our input data is a 2D tensor of shape (2, 4). The first dimension, of length 2, represents our two sentences, and the second dimension, of length 4, represents the words within each sentence. The '0' values are padding tokens that we've added to make each sentence the same length.\n",
    "\n",
    "During training, GPT-2 uses this 2D tensor as input into its self-attention mechanism. In a nutshell, self-attention allows the model to weigh the importance of each word within a sentence when predicting the next word. The weights are learned during training and depend on the context provided by the other words in the sentence. This is how GPT-2 is able to generate contextually relevant responses.\n",
    "\n",
    "___\n",
    "\n",
    "## Transcript Data\n",
    "\n",
    "Let's take a look at the dialogue example:\n",
    "\n",
    "| response\t| context\t| context/0\t| context/1\t| context/2\t| context/3\t| context/4\t| context/5 |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| What do you think of this... flying vehicle, M...\t| We gotta go, gotta get outta here, come on. Go... | Ow! Ow! You're tugging me too hard!\t| Come on, I got a surprise for you. Come on, h...\t| It's the middle of the night. What are you tal...\t| I got a surprise for you, Morty.\t| What, Rick? Whatâ€™s going on?\t| Morty! You gotta come on. Jus'... you gotta co... |\n",
    "\n",
    "Our first step in preparing this dialogue for input into our model is to concatenate all the context and response strings into one string for each row, adding an 'end of string' token between each response:\n",
    "\n",
    "```\n",
    "\"Morty! You gotta come on. Jus'... you gotta co... <eos> What, Rick? Whatâ€™s going on? <eos> I got a surprise for you, Morty. <eos> It's the middle of the night. What are you tal... <eos> Come on, I got a surprise for you. Come on, h... <eos> Ow! Ow! You're tugging me too hard! <eos> We gotta go, gotta get outta here, come on. Go... <eos> What do you think of this... flying vehicle, M...\"\n",
    "```\n",
    "\n",
    "Next, we use the DialoGPT tokenizer to tokenize this string and convert it into numerical form:\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "input_ids = tokenizer.encode(\n",
    "    \"Morty! You gotta come on. Jus'... you gotta co... <eos> What, Rick? Whatâ€™s going on? <eos> I got a surprise for you, Morty. <eos> It's the middle of the night. What are you tal... <eos> Come on, I got a surprise for you. Come on, h... <eos> Ow! Ow! You're tugging me too hard! <eos> We gotta go, gotta get outta here, come on. Go... <eos> What do you think of this... flying vehicle, M...\", \n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(input_ids.shape)\n",
    "```\n",
    "\n",
    "The output of this snippet should be something like `(1, N)`, where `N` is the number of tokens in our dialogue string. The 2D shape is because the tokenizer automatically adds an extra dimension for the batch size, even when we're only processing one dialogue.\n",
    "\n",
    "We confirm this with the shape being `(1, 132)` with the tensor:\n",
    "\n",
    "```python\n",
    "tensor([[   44,   419,    88,     0,   921, 17753,  1282,   319,    13,   449,\n",
    "           385,     6,   986,   345, 17753,   763,   986,  1279,    68,   418,\n",
    "            29,  1867,    11,  8759,    30,  1867,   447,   247,    82,  1016,\n",
    "           319,    30,  1279,    68,   418,    29,   314,  1392,   257,  5975,\n",
    "           329,   345,    11, 30395,    13,  1279,    68,   418,    29,   632,\n",
    "           338,   262,  3504,   286,   262,  1755,    13,  1867,   389,   345,\n",
    "          3305,   986,  1279,    68,   418,    29,  7911,   319,    11,   314,\n",
    "          1392,   257,  5975,   329,   345,    13,  7911,   319,    11,   289,\n",
    "           986,  1279,    68,   418,    29, 11960,     0, 11960,     0,   921,\n",
    "           821, 27762,  2667,   502,  1165,  1327,     0,  1279,    68,   418,\n",
    "            29,   775, 17753,   467,    11, 17753,   651,   503,  8326,   994,\n",
    "            11,  1282,   319,    13,  1514,   986,  1279,    68,   418,    29,\n",
    "          1867,   466,   345,   892,   286,   428,   986,  7348,  4038,    11,\n",
    "           337,   986]])\n",
    "```\n",
    "\n",
    "When we process a batch of dialogues, the shape would be `(B, N)`, where `B` is the batch size and `N` is the length of the longest dialogue in the batch. The other dialogues in the batch would be padded with zeros until they match this length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DialoGPT - A GPT-2 Variant Model Architecture\n",
    "\n",
    "The GPT-2 (Generative Pretrained Transformer 2) model, developed by OpenAI, is a large-scale transformer-based language model. It builds upon the architecture of the original GPT model, with improvements in the model size, training data, and parameters. The GPT-2 model is designed to generate human-like text by predicting the next word in a given sequence of words. This makes it particularly suitable for tasks like text generation, translation, summarization, and more. In the context of our assignment, the GPT-2 architecture forms the basis for the DialoGPT model, which we use to create a chatbot that mimics the character Rick from Rick and Morty.\n",
    "<img src=\"assets/dialogpt.png\" alt=\"dialogpt\" height=\"200\">\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "The Transformer model, introduced in the paper \"Attention is All You Need\" by Vaswani et al., is the backbone of the GPT-2 model. The Transformer model is based on a self-attention mechanism and does away with recurrence and convolutions entirely. This architecture allows the model to process input sequences in parallel, rather than sequentially, leading to significant improvements in training speed.\n",
    "\n",
    "The GPT-2 model uses a modified version of the Transformer, which only includes the decoder part of the original Transformer model. The decoder consists of a stack of identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. A residual connection is employed around each of the two sub-layers, followed by layer normalization.\n",
    "\n",
    "In our chatbot application, the Transformer architecture allows the model to generate responses in a conversational context, considering the entire context of the conversation rather than just the immediate previous response.\n",
    "<img src=\"assets/gpt2_architecture.png\" alt=\"gpt_architecture\" height=\"400\">\n",
    "## Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism, also known as scaled dot-product attention, is a key component of the Transformer architecture. It allows the model to weigh the importance of words in an input sequence when generating an output sequence. In other words, it helps the model to decide where to \"pay attention\" when generating text.\n",
    "\n",
    "In the context of our chatbot, the self-attention mechanism allows the model to generate responses that are contextually relevant and coherent. For example, if a user asks the chatbot a question about a specific episode of Rick and Morty, the self-attention mechanism helps the model to focus on the relevant parts of the conversation history when generating a response.\n",
    "\n",
    "<img src=\"assets/self_attention.webp\" alt=\"self_attention\" height=\"300\">\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "Positional encoding is used in the Transformer model to give the model some information about the relative positions of the words in the input sequence. Since the Transformer model doesn't have any recurrence or convolutions, it doesn't have any inherent sense of position or order of the words. Positional encoding solves this problem by adding a vector to each input embedding. These vectors follow a specific pattern that allows the model to determine the position of each word in the sequence.\n",
    "\n",
    "In the context of our chatbot, positional encoding helps the model to generate responses that make sense in the context of the conversation. For example, if a user asks a follow-up question, the model can use the positional encoding to understand that the follow-up question is related to the previous question, and generate a response accordingly.\n",
    "\n",
    "<img src=\"assets/positional_encoding.png\" alt=\"positional_encoding\" height=\"200\">\n",
    "\n",
    "## Layer Normalization\n",
    "\n",
    "Layer normalization is a technique used in the GPT-2 model to stabilize the neural network's learning process. It normalizes the inputs across the features instead of normalizing the features across the batch as in batch normalization. In other words, for each feature computed, layer normalization computes the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.\n",
    "\n",
    "In the context of our chatbot, layer normalization helps to stabilize the learning process and improve the model's performance. It ensures that the scale of the features does not change drastically, making the model more robust and less prone to overfitting. This is particularly important for a chatbot application, where the model needs to handle a wide variety of inputs and generate coherent and contextually relevant responses.\n",
    "\n",
    "<img src=\"assets/layer_norm.png\" alt=\"layer_norm\" height=\"200\">\n",
    "\n",
    "## Feed-Forward Networks\n",
    "\n",
    "Each layer of the GPT-2 model contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation function in between. While the linear transformations are the same across different positions, they use different parameters from layer to layer.\n",
    "\n",
    "The feed-forward networks in the GPT-2 model serve to increase the representational power of the model. They allow the model to learn more complex patterns in the data, which is crucial for a chatbot application. For example, the model needs to understand complex conversational patterns, detect sarcasm or humor, and generate responses that are not only contextually relevant but also in line with the character's personality (in this case, Rick from Rick and Morty).\n",
    "\n",
    "<img src=\"assets/feed_forward.png\" alt=\"feed_forward\" height=\"200\">\n",
    "\n",
    "## Model Size and Parameters\n",
    "\n",
    "The GPT-2 model comes in several sizes, ranging from \"small\" (117 million parameters) to \"extra large\" (1.5 billion parameters). The size of the model (i.e., the number of parameters) is a key factor that determines the model's capacity to learn from data. A larger model can learn more complex patterns in the data, but it also requires more computational resources to train and run.\n",
    "\n",
    "In the context of our chatbot, we need to balance the model size with the available computational resources and the complexity of the task. For example, if we want the chatbot to generate highly creative and nuanced responses that closely mimic the character Rick, we might opt for a larger model. However, if we have limited computational resources or if the chatbot's responses don't need to be highly complex, a smaller model might suffice.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The GPT-2 model, like many other language models, uses a loss function to measure the difference between its predictions and the actual outcomes. This loss function is crucial for training the model, as it provides a metric that the model can aim to minimize through gradient descent.\n",
    "\n",
    "In the case of GPT-2, the loss function used is the Cross-Entropy Loss. This is a common choice for classification problems, including language modeling, which can be thought of as the task of predicting the next word in a sequence (a form of classification).\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss, also known as Negative Log Likelihood, measures the dissimilarity between the model's predictions and the actual outcomes. It is calculated as follows:\n",
    "\n",
    "```\n",
    "Cross-Entropy Loss = - Î£ (y_i * log(p(y_i)))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `y_i` is the actual outcome (1 for the correct word, 0 for all other words in the vocabulary)\n",
    "- `p(y_i)` is the predicted probability of the outcome `y_i` according to the model\n",
    "\n",
    "The model outputs a probability distribution over the entire vocabulary for the next word in a sequence. The Cross-Entropy Loss is then calculated between this predicted distribution and the actual outcome (which is a one-hot encoded vector representing the actual next word).\n",
    "\n",
    "<img src=\"assets/loss.png\" alt=\"loss\" height=\"200\">\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DialoGPT and its Adaptation from GPT-2\n",
    "\n",
    "DialoGPT is a variant of the GPT-2 model, specifically fine-tuned for conversational responses. It was trained on a large dataset of internet-based conversations, allowing it to generate human-like conversational responses. This makes DialoGPT particularly suitable for chatbot applications, such as our Rick and Morty character chatbot.\n",
    "\n",
    "## Differences and Similarities with GPT-2\n",
    "\n",
    "While DialoGPT and GPT-2 share the same underlying Transformer architecture, there are some key differences between the two models, mainly due to the fine-tuning process used to train DialoGPT.\n",
    "\n",
    "### Similarities\n",
    "\n",
    "- **Architecture**: Both GPT-2 and DialoGPT are based on the Transformer architecture, which includes components like self-attention mechanism, positional encoding, layer normalization, and feed-forward networks.\n",
    "\n",
    "- **Training Method**: Both models are trained using a variant of the Transformer's decoder, with masked self-attention.\n",
    "\n",
    "- **Generative Models**: Both GPT-2 and DialoGPT are generative models, meaning they generate text by predicting the next word in a sequence.\n",
    "\n",
    "- **Loss Function**: Another similarity between GPT-2 and DialoGPT is the loss function used during training. Both models use the cross-entropy loss function, a common choice for training language models.\n",
    "\n",
    "    Cross-entropy loss measures the dissimilarity between the model's predictions and the actual data. In the context of language models like GPT-2 and DialoGPT, the model's prediction is the probability distribution of the next word in the sequence, and the actual data is the true next word.\n",
    "\n",
    "    The cross-entropy loss for a single prediction can be calculated using the formula:\n",
    "\n",
    "    ```\n",
    "    H(p, q) = - Î£ p(x) log(q(x))\n",
    "    ```\n",
    "\n",
    "    where:\n",
    "    - `p` is the true probability distribution (in this case, a one-hot encoded vector of the true next word)\n",
    "    - `q` is the predicted probability distribution (the model's output)\n",
    "    - `Î£` denotes the sum over all classes (words in the vocabulary)\n",
    "    - `log` is the natural logarithm\n",
    "\n",
    "    The goal during training is to minimize this loss, i.e., to make the model's predictions as close as possible to the actual data.\n",
    "\n",
    "    In the context of our chatbot, minimizing the cross-entropy loss means that the model is effectively learning to predict the next word in a dialogue, given the previous words. This allows the model to generate coherent and contextually appropriate responses, mimicking the character Rick from Rick and Morty.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Differences\n",
    "\n",
    "- **Training Data**: While GPT-2 is trained on a diverse range of internet text, DialoGPT is specifically fine-tuned on a dataset of internet dialogues. This allows DialoGPT to generate more conversational and contextually appropriate responses.\n",
    "\n",
    "- **Tokenization**: DialoGPT uses a byte-level BPE tokenizer, which allows it to handle a wider range of input text compared to GPT-2.\n",
    "\n",
    "- **Fine-Tuning**: DialoGPT is fine-tuned for dialogue generation tasks, which makes it more suitable for chatbot applications compared to GPT-2.\n",
    "\n",
    "In the context of our chatbot, the fine-tuning process allows DialoGPT to generate responses that are more conversational and contextually appropriate. This is crucial for creating a chatbot that can engage users in a natural and engaging conversation, mimicking the character Rick from Rick and Morty.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of DialoGPT Small and Large\n",
    "\n",
    "DialoGPT comes in different sizes, with the \"small\" and \"large\" variants being two of them. The primary difference between these variants lies in their capacity, which is determined by the number of parameters they have. The \"large\" variant has more parameters than the \"small\" one, which generally allows it to capture more complex patterns in the data.\n",
    "\n",
    "#### Effect on Results\n",
    "\n",
    "The increased capacity of the DialoGPT large model can lead to improved performance in generating text that mimics the character Rick Sanchez. It can potentially capture more nuanced aspects of Rick's personality and provide more contextually appropriate responses.\n",
    "\n",
    "However, the larger model also requires more computational resources to train and use, which can be a limiting factor depending on the available hardware. Additionally, larger models can sometimes overfit to the training data, especially if the dataset is small. This means that they might perform exceptionally well on the training data but struggle to generalize to new, unseen data.\n",
    "\n",
    "On the other hand, the DialoGPT small model, while less capable than the large model, is more computationally efficient. It can be trained and used on less powerful hardware, making it a more accessible option. However, its performance might not be as good as the large model, especially on complex tasks like mimicking a specific character's personality.\n",
    "\n",
    "In the context of our Rick Sanchez chatbot, choosing between the small and large DialoGPT models involves a trade-off between performance and computational efficiency. The best choice would depend on the specific requirements of the project, including the available hardware, the size of the training data, and the desired level of performance.\n",
    "\n",
    "For the majority of our training we confirmed that DialoGPT-small is indeed most appropriate for our project considering the performance in terms of robustness in capturing conversational nuances, and size of the model because of the limited time/computing resources. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Fine-Tuning DialoGPT\n",
    "\n",
    "After understanding the architecture and the underlying mechanisms of the GPT-2 and DialoGPT models, the next step is to delve into the practical aspects of implementing these models. Specifically, we'll focus on the training and fine-tuning process, which is crucial for adapting the pre-trained DialoGPT model to our specific task: creating a chatbot that mimics the character Rick from Rick and Morty.\n",
    "\n",
    "Fine-tuning is a process that involves further training a pre-trained model on a new dataset. In our case, this new dataset would ideally be a collection of dialogues from the Rick and Morty series. The fine-tuning process allows us to adapt the general language understanding capabilities of DialoGPT to the specific language style and nuances of the character Rick.\n",
    "\n",
    "In the following sections, we'll provide a step-by-step guide on how to fine-tune the DialoGPT model, including the code and explanations for each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-01T05:00:23.115643Z",
     "iopub.status.busy": "2023-08-01T05:00:23.115174Z",
     "iopub.status.idle": "2023-08-01T05:00:23.119709Z",
     "shell.execute_reply": "2023-08-01T05:00:23.119200Z",
     "shell.execute_reply.started": "2023-08-01T05:00:23.115617Z"
    },
    "executionInfo": {
     "elapsed": 55870,
     "status": "ok",
     "timestamp": 1690431684811,
     "user": {
      "displayName": "Honggyo Suh",
      "userId": "12671055514131838706"
     },
     "user_tz": -600
    },
    "id": "9CasAHUD-opl",
    "outputId": "8107bb62-0c85-4ff3-e903-5855ec066d3f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Python code on jupyter notebook to fine-tune language model (DialoGPT-large) of Microsoft\n",
    "so that it speaks in a certain style of Rick, who is the character from Rick and Morty sitcom.\n",
    "\n",
    "\n",
    "important libraries used:\n",
    "Pytorch (machine learning framework)\n",
    "sklearn (data splitting & preprocessing)\n",
    "Transformer (to download/fine-tune existing model)\n",
    "nltk (evaluation metrics)\n",
    "\"\"\"\n",
    "import glob, logging, pickle, random, re, shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T05:00:30.224438Z",
     "iopub.status.busy": "2023-08-01T05:00:30.223976Z",
     "iopub.status.idle": "2023-08-01T05:00:30.227726Z",
     "shell.execute_reply": "2023-08-01T05:00:30.227236Z",
     "shell.execute_reply.started": "2023-08-01T05:00:30.224415Z"
    },
    "id": "EHE7izJ0OxvH"
   },
   "outputs": [],
   "source": [
    "# Configuration code\n",
    "# Setting up a logger for this script so that the module write log messages to the console or a file\n",
    "logger = logging.getLogger(__name__)\n",
    "# Mapping provided by the HuggingFace transformers library that maps each model type to its corresponding configuration class\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "# Identifies the type of the model\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling Pipeline:\n",
    "\n",
    "We define the Args class, encapsulating essential hyperparameters and settings for our model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T05:00:48.444269Z",
     "iopub.status.busy": "2023-08-01T05:00:48.443840Z",
     "iopub.status.idle": "2023-08-01T05:00:48.449734Z",
     "shell.execute_reply": "2023-08-01T05:00:48.449098Z",
     "shell.execute_reply.started": "2023-08-01T05:00:48.444243Z"
    },
    "id": "caxuSdvUOtH4"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    \"\"\"\n",
    "    Argument class definition, define hyperparameters and setting for the model training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Place to store the trained model\n",
    "        self.output_dir = 'output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        # Model used for training\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        # Directory where the preprocessed datasets will be cached\n",
    "        self.cache_dir = 'cached-large'\n",
    "        # Maximum sequence length that this model might ever use\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        # Currently disabled, every evaluation will be done once the training is over\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        # Number of training epochs\n",
    "        self.num_train_epochs = 3\n",
    "        # Overrides above and used for new number of training if set to positive\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        # How often logging should happen\n",
    "        self.logging_steps = 1000\n",
    "        # How often checkpoints should be saved\n",
    "        self.save_steps = 300\n",
    "        self.save_total_limit = None\n",
    "        # Evaluate all checkpoints saved\n",
    "        self.eval_all_checkpoints = True\n",
    "        # Whether to use CUDA\n",
    "        self.no_cuda = False\n",
    "        # Overwrite the result by default\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        # Training initialisation seed\n",
    "        self.seed = 42\n",
    "        # For distributed training\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.123019Z",
     "iopub.status.busy": "2023-07-31T09:10:05.122796Z",
     "iopub.status.idle": "2023-07-31T09:10:05.127286Z",
     "shell.execute_reply": "2023-07-31T09:10:05.126278Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.123002Z"
    },
    "id": "PZbe4C7TOfHZ"
   },
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    \"\"\"\n",
    "    Used to set the seed for generating random numbers, ensure that the results are reproducible.\n",
    "    \"\"\"\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.128484Z",
     "iopub.status.busy": "2023-07-31T09:10:05.128296Z",
     "iopub.status.idle": "2023-07-31T09:10:05.134034Z",
     "shell.execute_reply": "2023-07-31T09:10:05.133044Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.128467Z"
    },
    "id": "ugq-9wAROfO8"
   },
   "outputs": [],
   "source": [
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    To get a list of saved model checkpoint directories in sorted order.\n",
    "    Can be used to find the most recent checkpoint.\n",
    "    \"\"\"\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.135459Z",
     "iopub.status.busy": "2023-07-31T09:10:05.134988Z",
     "iopub.status.idle": "2023-07-31T09:10:05.140135Z",
     "shell.execute_reply": "2023-07-31T09:10:05.139177Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.135441Z"
    },
    "id": "K82cyp8nOc34"
   },
   "outputs": [],
   "source": [
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    \"\"\"\n",
    "    To limit the total number of model checkpoints saved by deleting the oldest when the limit is exceeded.\n",
    "    \"\"\"\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # To check if current number of checkpoints exceed the limit\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training process, we prepare inputs and labels for each batch, calculate the loss, and adjust model parameters through backpropagation. We account for multiple GPUs and gradient accumulation, and use mixed precision training if needed. Key metrics are logged to TensorBoard, and after each step, we update the learning rate scheduler, reset gradients, and increment our global step counter, ensuring an efficient and effective training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Main function for training the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up Tensorboard writer, which is a tool for visualizing training metrics\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    # Calculates the total training batch size depend on the number of GPUs\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    # Define a function called 'collate' to handle the padding of input sequences.\n",
    "    # 'examples' is a list of tensors where each tensor represents a sequence of encoded tokens from the text.\n",
    "    # Note that sequences can have different lengths because the text entries are not of the same length.\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "\n",
    "        # Check if a pad token is defined in the tokenizer.\n",
    "        # Pad token is used to fill up sequences to the same length for efficient computation.\n",
    "        # The if condition is checking whether a pad token exists for this specific tokenizer.\n",
    "        if tokenizer._pad_token is None:\n",
    "\n",
    "            # If no pad token is defined in the tokenizer, we use the 'pad_sequence' function from PyTorch.\n",
    "            # This function pads sequences to the maximum length sequence in 'examples' with zeros by default.\n",
    "            # 'batch_first=True' ensures padding is applied to dimension 0, which represents different sequences in the batch.\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "\n",
    "        # If pad token exists in the tokenizer, we use the 'pad_sequence' function with the pad token's id.\n",
    "        # 'padding_value' parameter is set to 'tokenizer.pad_token_id', so sequences are padded with the pad token instead of zeros.\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "    # Creates a DataLoader for the training data\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # Calculates the total number of training steps and adjusts the number of epochs if necessary\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model\n",
    "    # Resizes the model's token embeddings to match the tokenizer's vocabulary size.\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # The following sets up the optimizer and learning rate scheduler for the model,\n",
    "    # taking into account whether to apply weight decay or not.\n",
    "\n",
    "    # 'Weight decay' is a regularization technique that helps to prevent the model from overfitting.\n",
    "    # This is done by adding a small multiple of all parameters directly to the loss function to \n",
    "    # keep the weights (parameters) small and the model less complex.\n",
    "\n",
    "    # 'no_decay' is a list that specifies the model parameters (weights) that should NOT be decayed.\n",
    "    # The weights of 'bias' and 'LayerNorm' layers do not need regularization and are often excluded \n",
    "    # from weight decay to avoid unnecessary computation. \n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "    # 'optimizer_grouped_parameters' specifies the settings of parameters for the optimizer.\n",
    "    # The model parameters are divided into two groups based on whether weight decay should be applied.\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            # The first group includes parameters where weight decay should be applied.\n",
    "            # This group includes all parameters except those specified in 'no_decay'.\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            # Apply weight decay specified in 'args'.\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            # The second group includes parameters where weight decay should NOT be applied.\n",
    "            # This group includes parameters specified in 'no_decay'.\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            # No weight decay is applied.\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # AdamW is an optimizer that's an extension of Adam, with a more correct weight decay implementation.\n",
    "    # It's used to update model parameters based on gradients computed during backpropagation.\n",
    "    # Learning rate, epsilon (for numerical stability), and weight decay are specified in the settings.\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon, no_deprecation_warning=True)\n",
    "\n",
    "    # 'scheduler' is used to reduce the learning rate over time, which can help model convergence.\n",
    "    # The learning rate is often decreased linearly after a certain number of 'warmup' steps.\n",
    "    # This scheduler provides a linear learning rate decay schedule.\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "\n",
    "    # Check if there exists a save for optimizer and scheduler and load\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))  # Check if optimizer state exists\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))  # Check if scheduler state exists\n",
    "    ):\n",
    "\n",
    "        # If both optimizer and scheduler states exist, load them.\n",
    "        # 'load_state_dict' is a PyTorch method used to load a modelâ€™s (or optimizerâ€™s/schedulerâ€™s) saved parameters.\n",
    "\n",
    "        # Loading the optimizer state\n",
    "        # The optimizer state consists of the values of optimization variables that the optimizer uses to update model parameters. \n",
    "        # These could be values like moving averages of gradients, which are crucial for optimizers like Adam.\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "\n",
    "        # Loading the scheduler state\n",
    "        # The scheduler state is used to determine the learning rate at any given step in the training process.\n",
    "        # This could be important for schedulers that decrease the learning rate over time.\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    # Multi-GPU mode\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed-training mode\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Logs information about the training setup, including the number of examples, epochs, batch size, total optimization steps\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    # Continue training from existing checkpoint\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    # Begin main training loop.\n",
    "    # The function 'zero_grad' resets the gradients to zero before starting backpropagation.\n",
    "    # Backpropagation is the method to calculate the gradient of the loss function with respect to the parameters.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Create a progress bar using 'trange' for visual feedback in the console.\n",
    "    # 'trange' is a tqdm function to instantly make a progress bar.\n",
    "    # 'epochs_trained' is the starting epoch number if you're resuming training, else it's 0.\n",
    "    # 'args.num_train_epochs' is the total number of epochs the model will train for.\n",
    "    # 'desc' sets the description displayed on the progress bar.\n",
    "    # 'disable' disables the progress bar if the script is not running on the main process (to avoid cluttering the console).\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "\n",
    "    # 'set_seed' ensures reproducibility by setting the seed for generating random numbers.\n",
    "    set_seed(args)\n",
    "\n",
    "    # Iterate over all epochs.\n",
    "    for _ in train_iterator:\n",
    "\n",
    "        # Create another progress bar for iterations within an epoch.\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "        # Iterate over each batch of data in the current epoch.\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # If resuming from the last training, skip the initial steps that were already trained.\n",
    "            # 'steps_trained_in_current_epoch' is a variable keeping track of the number of steps trained in the current epoch.\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            # The training data and labels are the same in the case of language models like GPT.\n",
    "            # Here, the model predicts the next word, so the input sequence and target sequence are identical, only shifted by one position.\n",
    "            inputs, labels = (batch, batch)\n",
    "\n",
    "            # In some cases, the sequence length might exceed the model's maximum length (1024 for GPT-2).\n",
    "            # This line skips such sequences.\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "\n",
    "            # '.to(args.device)' ensures that the input and label tensors are on the correct device (GPU or CPU), \n",
    "            # based on the configuration settings.\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "\n",
    "           # Change the model to training mode.\n",
    "            # This activates the training-specific operations like dropout.\n",
    "            model.train()\n",
    "\n",
    "            # Perform a forward pass through the model.\n",
    "            # This generates the output predictions from the input sequences.\n",
    "            # 'labels' are provided because this is a supervised learning task.\n",
    "            outputs = model(inputs, labels=labels)\n",
    "\n",
    "            # The first element of 'outputs' is the loss - a measure of how far the model's predictions are from the actual data.\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # If using multiple GPUs, take the mean of the losses.\n",
    "            # The loss is calculated separately on each GPU for different portions of the data.\n",
    "            # The mean loss provides a single value to optimize.\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            # If gradient accumulation is enabled, divide the loss by the accumulation steps.\n",
    "            # This is because the gradients from each step are added together,\n",
    "            # and so the effective learning rate would be 'args.gradient_accumulation_steps' times higher than the set value.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            # Backward propagation (backpropagation) is the process of working backwards through the training process.\n",
    "            # It calculates the gradient, or the partial derivative of the loss function with respect to the parameters.\n",
    "            if args.fp16:\n",
    "                # When using 16-bit floating-point numbers (half-precision), use 'amp.scale_loss' to adjust the loss value.\n",
    "                # This helps prevent numerical underflow (numbers too small to be represented in the chosen format).\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                # For full-precision training (32-bit floating-point numbers), just perform a regular backward pass.\n",
    "                loss.backward()\n",
    "\n",
    "            # Accumulate the training loss for logging.\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            # If gradient accumulation steps have completed, perform a step of optimization.\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # Clip the gradient to avoid exploding gradients problem.\n",
    "                # This is where the gradients become too large and cause the learning process to fail.\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                # 'optimizer.step()' updates the parameters based on the gradients.\n",
    "                optimizer.step()\n",
    "\n",
    "                # 'scheduler.step()' updates the learning rate for the next step.\n",
    "                scheduler.step()\n",
    "\n",
    "                # After updating the parameters, reset the gradients back to zero for the next batch.\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Keep track of the total number of steps taken.\n",
    "                global_step += 1\n",
    "\n",
    "\n",
    "            # Logs the training loss and learning rate to Tensorboard\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Logs the evaluation metrics to Tensorboard\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    if (args.local_rank == -1 and args.evaluate_during_training):\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                # Save model checkpoint depends on save_steps parameter\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates our model's performance on the validation dataset, calculating the average loss and perplexity, which are then logged and saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.162711Z",
     "iopub.status.busy": "2023-07-31T09:10:05.162525Z",
     "iopub.status.idle": "2023-07-31T09:10:05.170998Z",
     "shell.execute_reply": "2023-07-31T09:10:05.170128Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.162694Z"
    },
    "id": "J4XxBBZGN_-1"
   },
   "outputs": [],
   "source": [
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Evalutates the performance of the model with validation dataset.\n",
    "    \"\"\"\n",
    "    # Place that the evaluation results to be saved\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    # Load the processed data\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    # Dataloader for the evaluation data\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # Multi-GPU mode\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Loggs information about the evaluation settings\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    # Change to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Main evaluation loop, iterates over all batches of data. Each batch is processed by the model\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Calculates the average loss and the perplexity of the model on the evaluation data\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    # Writes the evaluation results to a file and also logs them\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code and explain what each part does in the context of fine-tuning the DialoGPT model.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "```\n",
    "This is the main function for evaluating the model's performance on the validation dataset. It takes as input the model, tokenizer, training and validation data, and a prefix string for naming the output files.\n",
    "\n",
    "```python\n",
    "eval_output_dir = args.output_dir\n",
    "```\n",
    "This line sets the directory where the evaluation results will be saved.\n",
    "\n",
    "```python\n",
    "eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "```\n",
    "This line loads the validation data and processes it into a format that can be used by the model. The `load_and_cache_examples` function tokenizes the text and converts it into tensors.\n",
    "\n",
    "```python\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "```\n",
    "This line sets the batch size for evaluation. The batch size is the number of examples that are processed at the same time. The total batch size is the per-GPU batch size multiplied by the number of GPUs.\n",
    "\n",
    "```python\n",
    "def collate(examples: List[torch.Tensor]):\n",
    "    if tokenizer._pad_token is None:\n",
    "        return pad_sequence(examples, batch_first=True)\n",
    "    return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "```\n",
    "This function is used to combine the examples into batches. It pads the sequences so that they all have the same length.\n",
    "\n",
    "```python\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    ")\n",
    "```\n",
    "These lines create a dataloader for the validation data. The dataloader is a PyTorch object that efficiently loads the data in batches.\n",
    "\n",
    "```python\n",
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "```\n",
    "If there are multiple GPUs available, this line wraps the model in a `DataParallel` object to enable parallel processing over multiple GPUs.\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "This line sets the model to evaluation mode. In this mode, the model's parameters are not updated.\n",
    "\n",
    "```python\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    inputs, labels = (batch, batch)\n",
    "    inputs = inputs.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "```\n",
    "This is the main evaluation loop. It iterates over all batches of data. Each batch is processed by the model, and the model's outputs are compared to the actual labels to compute the loss.\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    lm_loss = outputs[0]\n",
    "    eval_loss += lm_loss.mean().item()\n",
    "```\n",
    "These lines compute the model's loss on the current batch. The `torch.no_grad()` context manager is used to prevent PyTorch from building a computational graph for these operations, which saves memory.\n",
    "\n",
    "```python\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "result = {\"perplexity\": perplexity}\n",
    "```\n",
    "These lines compute the average loss and the perplexity of the model on the validation data. Perplexity is a common metric for evaluating language models; lower perplexity means better performance.\n",
    "\n",
    "```python\n",
    "output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "```\n",
    "These lines write the evaluation results to a file and also log them.\n",
    "\n",
    "---\n",
    "\n",
    "This function is a crucial part of the fine-tuning process, it allows us to monitor the model's performance and make sure that it's improving over time. This function helps us ensure that the model is learning to generate responses that are similar to the character Rick from Rick and Morty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.172338Z",
     "iopub.status.busy": "2023-07-31T09:10:05.171915Z",
     "iopub.status.idle": "2023-07-31T09:10:05.182821Z",
     "shell.execute_reply": "2023-07-31T09:10:05.181982Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.172317Z"
    },
    "id": "eAY_eIj0-zrc"
   },
   "outputs": [],
   "source": [
    "def main(df_trn, df_val):\n",
    "    \"\"\"\n",
    "    Entry point of the script, responsible for training and evaluation processes.\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "\n",
    "    # Whether to continue from previous training and/or checkpoint\n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Determines the device to use for training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Sets up the logging configuration to track the progress of the training and can be useful for debugging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    set_seed(args)\n",
    "    # Model and tokenizer initialization\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Load the training data and calls the train function\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saves the model, configuaration, arguments and the tokenizer to the output directory\n",
    "    if args.do_train:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load the finetuned model\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Load the validation data and calls the evaluation function\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "\n",
    "        # Main evaluation loop iterates checkpoints to be evaluated\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            print(checkpoint)\n",
    "            print(global_step)\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            results[f\"perplexity_{global_step}\"] = result[\"perplexity\"]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611,
     "referenced_widgets": [
      "7ce699e145d44c00b05255064ed081b5",
      "3dda1b6a75ff41bfa98e2a182531391e",
      "b408f7cdb7ed43a88d750297211c7cec",
      "cfdedad418cf462a898625da74430389",
      "64045879d6244f1b915a2a695260e3b2",
      "5aefd7880e7f414f87fef00c0c1c1127",
      "d6e20e90f3fc40d0afe7aececa274384",
      "abb866e3f37b41bea9b2631de916af75",
      "67e89c39ccf6470eb233a51c4548ba7b",
      "7b694cc2a19947308d4aa8c1519ebf0b",
      "8d9f18b04db943a48e8fe49ac25971f9",
      "567a23152ed04da1830acbe2dcbdd551",
      "57d7f1e41107467e9f4dd23fed8dacba",
      "d360fa90ece4429eb93552a8d84c6f0b",
      "69937e99baa044b1851a5f0cc48f010b",
      "1f578e94f5f745db8a85c02355db6abc",
      "3c8855a72f4a4e2fa2f753394f5abf4b",
      "f5087d42980149f58c3f0361c10211cc",
      "b3734cc835c24877b0c750a85c9d2389",
      "aac30f978b3b403082842bf8ee457fdd",
      "b9ce25f5495e45678a56db0cbef5551a",
      "f6e11ac987d944a79b909f0ba6c33f96",
      "978e6fd6031b4f44a58f0bd4bc5b3384",
      "85856d61843543578471b132e7b6f351",
      "e300caa213f44ac78522ce11d9778ad1",
      "021c708a7d674c28a4263d0713618bd6",
      "3f88d37fc7fe41048ccd050b533f50e2",
      "99da047371e4417e81746e3d9f86b08c",
      "21f01bbc75674755b30d118d764ef62b",
      "fc0bb43ea8d94fba9d70e98151bbc031",
      "f561c589bd1146c0aed045b6e605b4d3",
      "6e07a10962c042299a4ab98a3f8114ef",
      "b25c4d9ebf424ec9837ddbedf8e82dd2",
      "ca13c898fa4c40a7a68d7e27e2b4a919",
      "7e5742a43fd8465ca8aad04caaad0933",
      "1b85a478df2b47dbb46317977b4d9390",
      "0f7f5165832d4a9bbfabf0f2176ab17f",
      "64f1adda69124d4aae77de31242b6513",
      "12e01f310b2e47eba2892af875a5e997",
      "55c977d6c0384547a42a69e9ec135a67",
      "36ff813827f24321a909985f41cc8667",
      "89965f0fad1244c8bf68fc548bee380a",
      "03c631b6de0d44248c48119761b09f9d",
      "933946ee0bd449068e53c3b8bacbd93c",
      "51bba57884ee45abbf964d2d80bd1cda",
      "364d8271f76348868de1979bae48bf39",
      "91bf6335838048d8830690a579262c16",
      "b6c957478a55470ca26e6c6cbe756100",
      "3149e4051f5646848d02f64ed49d830d",
      "2689998d52004a4db12d5833422e6b67",
      "68b718c3fe7643788b982f024927ec1f",
      "616c1d99b2724a6fbddbdc9abbf18718",
      "a8bb6ac84a704c2fa3c027fa0e6c0fe6",
      "1a0b0be7e4714ae790d95a92a8078902",
      "bfd072939aa445f2bdb46edfe89c94c5",
      "9c76517b1dde4535a92a50e42b600149",
      "13a1e1b4753b4a1a9fa362d19489189f",
      "def406c0c91d4e8e83157005ccba4c51",
      "417eb867d1b844bba8211607c40580f8",
      "28e407e0aa4a48c6a9d4ff65938e6961",
      "92473f667fa44d3594b85a560c33202b",
      "f35ad2d448d249f5967fa324be79b941",
      "4e9269cdb1b048dab84a7df39f9fec3c",
      "a52cba19ab9245628f085368052e737e",
      "e884c11cf53441bf96a0deb949696662",
      "cc4ef5e0da494d6fad9ec561384e2f1b",
      "1d968c385299408197a34e38300deaa0",
      "dddf76fde26c4f4b8c7e18d3fbea05fe",
      "2cd0142c10504db88bf133d9341ccd72",
      "c70afbc9cafd45be9c2823a675699cc2",
      "8442f2829cce4045a8fbf35ea028959c",
      "c06483936c004b2394a153c92738b996",
      "0b58496b39ba453dbe732b3baaf62abd",
      "e795e8b6735c41cb8945bb1cf395d965",
      "9a99d7009e604b26a0f90ba4ab4f22e5",
      "4afaa8bf0c7c431db678d63af61b0312",
      "42a2c22d78634f7aa101218662a297df",
      "31925a815dfb418b8bda148e7fb7614a",
      "08980302545a4abcad9b6a96f5d036e9",
      "24d36cdd607e47bd83d1ce630d4d0186",
      "42d13228493646568c5f09da675ba4e8",
      "fc53c5a6f20e43fc92e1421a47ea7bde",
      "8044cdda3bcf47c287a7bfd23036910f",
      "978f389524e743219c6887fd351782a2",
      "c6b83442e10142e9b3a1253422d2021b",
      "85fa025cc05e4b67a3f5627422af4409",
      "756c40218695403b958412a88328f53f",
      "1254cb3e5f9a450983e4a484f4a3112e",
      "1c4b7b92ed5848d6b96c59d81d41264b",
      "dc35abe55fdf4c3db99beccf34b781c2",
      "345b01f1bc7b4ea1bcad2c1c5d8fee8b",
      "6b4251a0852043b7afe9567f2220231c",
      "014c0f4f80234bd2b42a5b127560156c",
      "7802d80e667e469e846e55ec345b603e",
      "2cf04f43cdb64abebd20bea5de53cbf5",
      "c8e9526b0d174f5aa7c99ad053972b49",
      "15f8172dc94341f3b1f1ccb81d064023",
      "3785ef667d2744c68bcc9b2f1f78260f",
      "77622ffe28ee47539e2a2738de80d411"
     ]
    },
    "execution": {
     "iopub.execute_input": "2023-07-31T09:10:05.185108Z",
     "iopub.status.busy": "2023-07-31T09:10:05.184716Z",
     "iopub.status.idle": "2023-07-31T09:22:26.508745Z",
     "shell.execute_reply": "2023-07-31T09:22:26.505486Z",
     "shell.execute_reply.started": "2023-07-31T09:10:05.185089Z"
    },
    "executionInfo": {
     "elapsed": 451965,
     "status": "ok",
     "timestamp": 1690432686940,
     "user": {
      "displayName": "Honggyo Suh",
      "userId": "12671055514131838706"
     },
     "user_tz": -600
    },
    "id": "sHZ09DtK-x4N",
    "outputId": "8be3735f-9f95-489f-c1ee-9addcb76e018"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:10:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5466b075fd7247e68d5b7643eb2e814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49920b8aec194875a0e8caa87de0e1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eb53b4088f4041b4e2cc76227e8447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f4b7de51074906a67640ba1a36d673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b484536ad42a407788043e0c65f42e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:10:42 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f86806cd0d0>\n",
      "07/31/2023 09:10:42 - INFO - __main__ -   Creating features from dataset file at cached-large\n",
      "07/31/2023 09:10:43 - INFO - __main__ -   Saving features into cached file cached-large/gpt2_cached_lm_512\n",
      "07/31/2023 09:10:43 - INFO - __main__ -   ***** Running training *****\n",
      "07/31/2023 09:10:43 - INFO - __main__ -     Num examples = 1708\n",
      "07/31/2023 09:10:43 - INFO - __main__ -     Num Epochs = 3\n",
      "07/31/2023 09:10:43 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "07/31/2023 09:10:43 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "07/31/2023 09:10:43 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/31/2023 09:10:43 - INFO - __main__ -     Total optimization steps = 1281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976edaf6aa994755b6d021d2fd9822fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b5f44930974da08d13de092d4e3075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:13:03 - INFO - __main__ -   Saving model checkpoint to output-large/checkpoint-300\n",
      "07/31/2023 09:13:10 - INFO - __main__ -   Saving optimizer and scheduler states to output-large/checkpoint-300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3246400b6e064f1f84ff5cd5efd09f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:15:29 - INFO - __main__ -   Saving model checkpoint to output-large/checkpoint-600\n",
      "07/31/2023 09:15:36 - INFO - __main__ -   Saving optimizer and scheduler states to output-large/checkpoint-600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f680c0acb04f3898b32826589402e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:17:54 - INFO - __main__ -   Saving model checkpoint to output-large/checkpoint-900\n",
      "07/31/2023 09:18:01 - INFO - __main__ -   Saving optimizer and scheduler states to output-large/checkpoint-900\n",
      "07/31/2023 09:20:21 - INFO - __main__ -   Saving model checkpoint to output-large/checkpoint-1200\n",
      "07/31/2023 09:20:28 - INFO - __main__ -   Saving optimizer and scheduler states to output-large/checkpoint-1200\n",
      "07/31/2023 09:21:04 - INFO - __main__ -    global_step = 1281, average loss = 0.7405934791567035\n",
      "07/31/2023 09:21:04 - INFO - __main__ -   Saving model checkpoint to output-large\n",
      "07/31/2023 09:21:17 - INFO - __main__ -   Evaluate the following checkpoints: ['output-large/checkpoint-1200', 'output-large/checkpoint-300', 'output-large/checkpoint-600', 'output-large/checkpoint-900', 'output-large']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-large/checkpoint-1200\n",
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:21:22 - INFO - __main__ -   Creating features from dataset file at cached-large\n",
      "07/31/2023 09:21:23 - INFO - __main__ -   Saving features into cached file cached-large/gpt2_cached_lm_512\n",
      "07/31/2023 09:21:23 - INFO - __main__ -   ***** Running evaluation checkpoint-1200 *****\n",
      "07/31/2023 09:21:23 - INFO - __main__ -     Num examples = 190\n",
      "07/31/2023 09:21:23 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3685e16595154483aea166ddea52f3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:21:28 - INFO - __main__ -   ***** Eval results checkpoint-1200 *****\n",
      "07/31/2023 09:21:28 - INFO - __main__ -     perplexity = tensor(1.0979)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-large/checkpoint-300\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:21:40 - INFO - __main__ -   Creating features from dataset file at cached-large\n",
      "07/31/2023 09:21:40 - INFO - __main__ -   Saving features into cached file cached-large/gpt2_cached_lm_512\n",
      "07/31/2023 09:21:40 - INFO - __main__ -   ***** Running evaluation checkpoint-300 *****\n",
      "07/31/2023 09:21:40 - INFO - __main__ -     Num examples = 190\n",
      "07/31/2023 09:21:40 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d4cd88bfaa47c49181610bf43fce2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:21:45 - INFO - __main__ -   ***** Eval results checkpoint-300 *****\n",
      "07/31/2023 09:21:45 - INFO - __main__ -     perplexity = tensor(2.7415)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-large/checkpoint-600\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:21:56 - INFO - __main__ -   Creating features from dataset file at cached-large\n",
      "07/31/2023 09:21:56 - INFO - __main__ -   Saving features into cached file cached-large/gpt2_cached_lm_512\n",
      "07/31/2023 09:21:56 - INFO - __main__ -   ***** Running evaluation checkpoint-600 *****\n",
      "07/31/2023 09:21:56 - INFO - __main__ -     Num examples = 190\n",
      "07/31/2023 09:21:56 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df62d48e5c784864be23f369c616379e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:22:02 - INFO - __main__ -   ***** Eval results checkpoint-600 *****\n",
      "07/31/2023 09:22:02 - INFO - __main__ -     perplexity = tensor(1.4079)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-large/checkpoint-900\n",
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:22:09 - INFO - __main__ -   Creating features from dataset file at cached-large\n",
      "07/31/2023 09:22:09 - INFO - __main__ -   Saving features into cached file cached-large/gpt2_cached_lm_512\n",
      "07/31/2023 09:22:09 - INFO - __main__ -   ***** Running evaluation checkpoint-900 *****\n",
      "07/31/2023 09:22:09 - INFO - __main__ -     Num examples = 190\n",
      "07/31/2023 09:22:09 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d88b72351a4d09a2fc811ad22b6c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:22:14 - INFO - __main__ -   ***** Eval results checkpoint-900 *****\n",
      "07/31/2023 09:22:14 - INFO - __main__ -     perplexity = tensor(1.1442)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-large\n",
      "large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:22:21 - INFO - __main__ -   Creating features from dataset file at cached-large\n",
      "07/31/2023 09:22:21 - INFO - __main__ -   Saving features into cached file cached-large/gpt2_cached_lm_512\n",
      "07/31/2023 09:22:21 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/31/2023 09:22:21 - INFO - __main__ -     Num examples = 190\n",
      "07/31/2023 09:22:21 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da6922caa3e4bf1b5ae8ce1761b3419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2023 09:22:26 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/31/2023 09:22:26 - INFO - __main__ -     perplexity = tensor(1.0951)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_1200': tensor(1.0979),\n",
       " 'perplexity_300': tensor(2.7415),\n",
       " 'perplexity_600': tensor(1.4079),\n",
       " 'perplexity_900': tensor(1.1442),\n",
       " 'perplexity_large': tensor(1.0951)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the main function, train and evaluate the model\n",
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T05:01:03.491406Z",
     "iopub.status.busy": "2023-08-01T05:01:03.490565Z",
     "iopub.status.idle": "2023-08-01T05:01:20.107551Z",
     "shell.execute_reply": "2023-08-01T05:01:20.106851Z",
     "shell.execute_reply.started": "2023-08-01T05:01:03.491378Z"
    },
    "id": "FF4sJsdtp06H"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2456c2da22364ffc8b9caf974dbb4976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the finetuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained('output-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-31T09:22:32.106292Z",
     "iopub.status.busy": "2023-07-31T09:22:32.106009Z",
     "iopub.status.idle": "2023-07-31T09:22:32.480485Z",
     "shell.execute_reply": "2023-07-31T09:22:32.479872Z",
     "shell.execute_reply.started": "2023-07-31T09:22:32.106274Z"
    },
    "executionInfo": {
     "elapsed": 1497,
     "status": "ok",
     "timestamp": 1690432755824,
     "user": {
      "displayName": "Honggyo Suh",
      "userId": "12671055514131838706"
     },
     "user_tz": -600
    },
    "id": "Um4FxjEhwvMV",
    "outputId": "ea04467b-21ae-4624-dc53-d8cb0d28a201"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T05:04:06.397156Z",
     "iopub.status.busy": "2023-08-01T05:04:06.396431Z",
     "iopub.status.idle": "2023-08-01T05:04:06.401615Z",
     "shell.execute_reply": "2023-08-01T05:04:06.401033Z",
     "shell.execute_reply.started": "2023-08-01T05:04:06.397132Z"
    },
    "id": "99EH7jm2xBHU"
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt_text, flag):\n",
    "    \"\"\"\n",
    "    Function to generate a response from the model based on a given prompt\n",
    "    \"\"\"\n",
    "    # Encode the input, append the eos token to signal the end of sequence, and converted to Pytorch tensor\n",
    "    new_user_input_ids = tokenizer.encode(prompt_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # Check if the input is given from the user or prepared prompt\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if flag and step > 0 else new_user_input_ids\n",
    "\n",
    "    # Generates a response to the input\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "\n",
    "    # Output decoded back into text\n",
    "    result = \"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-31T09:24:59.398173Z",
     "iopub.status.busy": "2023-07-31T09:24:59.397609Z",
     "iopub.status.idle": "2023-07-31T09:25:19.497576Z",
     "shell.execute_reply": "2023-07-31T09:25:19.496815Z",
     "shell.execute_reply.started": "2023-07-31T09:24:59.398144Z"
    },
    "executionInfo": {
     "elapsed": 6134,
     "status": "ok",
     "timestamp": 1690432766906,
     "user": {
      "displayName": "Honggyo Suh",
      "userId": "12671055514131838706"
     },
     "user_tz": -600
    },
    "id": "OK_KyTFCp4aV",
    "outputId": "d79f8e02-b7b3-49e0-b52d-16f96ebb2b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: How are you, Rick?\n",
      "generated: RickBot: Good. You?\n",
      "BLEU: 0.11362193664674995\n",
      "METEOR: 0.2380952380952381\n",
      "prompt: Play a song for me\n",
      "generated: RickBot: Rick, you're doing this bit while your brain is melting.\n",
      "BLEU: 0.02950234363196405\n",
      "METEOR: 0.21276595744680854\n",
      "prompt: What do you think about Elon Musk?\n",
      "generated: RickBot: He's okay.\n",
      "BLEU: 0.11362193664674995\n",
      "METEOR: 0.049019607843137254\n"
     ]
    }
   ],
   "source": [
    "# Prepare reference data from validation dataframe\n",
    "references = val_df['response'].tolist()\n",
    "prompt_texts = [\"How are you, Rick?\", \"Play a song for me\", \"What do you think about Elon Musk?\"]\n",
    "\n",
    "# Evaluate BLEU and METEOR score for each prompt\n",
    "for prompt in prompt_texts:\n",
    "    generated_text = generate_text(prompt, False)\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    print(f\"generated: {generated_text}\")\n",
    "    reference_tokens = [reference_text.split() for reference_text in references]\n",
    "    generated_tokens = [generated_text.split()]\n",
    "\n",
    "    # Calculate the BLEU score\n",
    "    chencherry = SmoothingFunction()\n",
    "    bleu_score = corpus_bleu([reference_tokens], generated_tokens, smoothing_function=chencherry.method1)\n",
    "    print(f\"BLEU: {bleu_score}\")\n",
    "\n",
    "    # Calculate the METEOR score\n",
    "    meteor_scores = [meteor_score([reference], generated_tokens[0]) for reference in reference_tokens]\n",
    "    max_meteor_score = max(meteor_scores)\n",
    "    print(f\"METEOR: {max_meteor_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with standard model (Not Finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T04:57:39.802858Z",
     "iopub.status.busy": "2023-08-01T04:57:39.802311Z",
     "iopub.status.idle": "2023-08-01T04:59:00.791291Z",
     "shell.execute_reply": "2023-08-01T04:59:00.790709Z",
     "shell.execute_reply.started": "2023-08-01T04:57:39.802833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I am the one who knocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Do you know Morty?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I do not.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: why don't aliens ever visit our planet?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Because they're not real.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: What if the universe is infinite, surely there are aliens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I don't know.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: What do you know?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I don't know.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained Model Performance\n",
    "\n",
    "The untrained DialoGPT model is a powerful language model capable of generating human-like text. However, without fine-tuning, it lacks the specific personality traits and knowledge of the character Rick Sanchez. Let's evaluate its performance based on the given dialogues:\n",
    "\n",
    "#### Coherence\n",
    "\n",
    "The model's responses are generally coherent in the sense that they form grammatically correct and complete sentences. However, they lack coherence in the context of the conversation. For example, the model's response \"I am the one who knocks\" to the question \"Who are you?\" is a coherent sentence, but it doesn't make sense in the context of the conversation.\n",
    "\n",
    "#### Relevance\n",
    "\n",
    "The model's responses are not always relevant to the questions asked. For instance, when asked \"Do you know Morty?\", the model responds with \"I do not.\" This is not relevant since Rick Sanchez, the character the model is supposed to mimic, is Morty's grandfather and they share a close relationship in the series.\n",
    "\n",
    "#### Personality\n",
    "\n",
    "The model does not exhibit the personality traits of Rick Sanchez. Rick is known for his sarcastic and cynical personality, his scientific knowledge, and his disregard for social norms. The model's responses do not reflect these traits.\n",
    "\n",
    "#### Robustness\n",
    "\n",
    "The model is robust in the sense that it can generate a response to any input. However, the quality and relevance of the responses can vary significantly, as seen in the dialogues.\n",
    "\n",
    "#### Context Awareness\n",
    "\n",
    "The model lacks context awareness. For example, it doesn't recognize the relationship between Rick and Morty, and it doesn't provide a thoughtful response to the question about the possibility of aliens in an infinite universe.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The untrained DialoGPT model, while solid, does not effectively simulate the personality of Rick Sanchez. It lacks context awareness, doesn't always provide relevant responses, and doesn't exhibit Rick's distinctive personality traits. This highlights the importance of fine-tuning the model on specific dialogues from the Rick and Morty series to create a more accurate and engaging Rick Sanchez chatbot. Fine-tuning should help the model learn the nuances of Rick's personality and improve its performance in all the evaluated areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-31T09:53:51.704035Z",
     "iopub.status.busy": "2023-07-31T09:53:51.703536Z",
     "iopub.status.idle": "2023-07-31T09:56:39.704281Z",
     "shell.execute_reply": "2023-07-31T09:56:39.703756Z",
     "shell.execute_reply.started": "2023-07-31T09:53:51.704013Z"
    },
    "executionInfo": {
     "elapsed": 76156,
     "status": "ok",
     "timestamp": 1690357231860,
     "user": {
      "displayName": "Honggyo Suh",
      "userId": "12671055514131838706"
     },
     "user_tz": -600
    },
    "id": "RCjE8OMj-rzg",
    "outputId": "57e3eae6-682b-48ae-ee31-283495d654e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Hey, Rick, why don't aliens ever visit our planet?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: Because our government keeps destroying their interdimensional portals and we're too busy building giant robots to care.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Giant robots, Rick? Seriously? Where are they hiding those?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: In my balls.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Uh, gross, Rick! Can't you just be serious for once?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: I'm serious. And don't call me that!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Rick, that's your name! What else am I supposed to call you?!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: Sweetie, could I get Get that syringe now?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Oh geez, Rick, what syringe? You're not going to experiment on me again, are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: Oh, man, Rick. I can't believe we're sitting around, standing around in Mr. Goldenfold's house. It's really weird.\n"
     ]
    }
   ],
   "source": [
    "# Chat with Rickbot for 5 conversation\n",
    "for step in range(5):\n",
    "    print(generate_text(input(\">> User:\"), False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Model Performance\n",
    "\n",
    "The fine-tuned DialoGPT model, after being trained on dialogues from the Rick and Morty series, shows a significant improvement in simulating the personality of Rick Sanchez. Let's evaluate its performance based on the given dialogues:\n",
    "\n",
    "#### Coherence\n",
    "\n",
    "The model's responses are coherent and form complete sentences. They also show improved coherence in the context of the conversation. For example, when asked about the existence of giant robots, the model responds with a humorous and sarcastic remark, which is in line with Rick's character.\n",
    "\n",
    "#### Relevance\n",
    "\n",
    "The model's responses are relevant to the questions asked. For instance, when asked about aliens visiting our planet, the model provides a creative and humorous response that fits within the sci-fi theme of Rick and Morty.\n",
    "\n",
    "#### Personality\n",
    "\n",
    "The model exhibits the personality traits of Rick Sanchez. It captures Rick's sarcastic humor, his disregard for social norms, and his scientific knowledge. For example, the model's response \"Because our government keeps destroying their interdimensional portals and we're too busy building giant robots to care\" captures Rick's cynical and sarcastic personality.\n",
    "\n",
    "#### Robustness\n",
    "\n",
    "The model is robust and can generate a response to any input. The quality and relevance of the responses have improved significantly after fine-tuning, as seen in the dialogues.\n",
    "\n",
    "#### Context Awareness\n",
    "\n",
    "The model shows improved context awareness. It maintains the context of the conversation and generates responses that are relevant to the previous dialogues. However, there are still moments where the model loses track of the context, such as the sudden mention of a syringe and Mr. Goldenfold's house.\n",
    "\n",
    "### Limitations and Influence of the Dataset\n",
    "\n",
    "One notable issue is the occasional loss of coherence, particularly in maintaining the persona of Rick Sanchez consistently throughout the conversation.\n",
    "\n",
    "For instance, in the last line of the dialogue, the model says, \"Oh, man, Rick. I can't believe we're sitting around, standing around in Mr. Goldenfold's house. It's really weird.\" This response is problematic because the model, which is supposed to be simulating Rick, refers to Rick in the third person, indicating a loss of persona coherence.\n",
    "\n",
    "This issue could be influenced by the diversity of dialogues in the training dataset. If the dataset contains dialogues from multiple characters, the model might learn to switch between different personas. While this could be beneficial in a multi-character chatbot, it's not desirable in our case, where the model is supposed to consistently simulate a single character, Rick Sanchez.\n",
    "\n",
    "To mitigate this issue, we could refine the dataset to include only dialogues where Rick is speaking. This would help the model learn to maintain Rick's persona more consistently. Additionally, we could experiment with techniques like persona conditioning, where additional information about the persona is provided to the model to help it maintain the persona more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T05:04:08.796657Z",
     "iopub.status.busy": "2023-08-01T05:04:08.795990Z",
     "iopub.status.idle": "2023-08-01T05:05:46.303208Z",
     "shell.execute_reply": "2023-08-01T05:05:46.302430Z",
     "shell.execute_reply.started": "2023-08-01T05:04:08.796631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: Some random guy with a gun.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Do you know Morty?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: What? N-No, I don't know Morty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Isn't he your grandson?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: He is my grandson.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Then why did you lie?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: Because I don't respect therapy, because I'm a scientist. Because I invent, transform, create, and destroy for a living, and when I donâ€™t like something about the world, I change it. And I don-t think going to a rented office in a strip mall to listen to some agent of averageness explain which words mean which feelings has ever helped anyone do anything.  I think it has helped a lot of people get comfortable and stop panicking, which is a state of mind we value in the animals we eat, but not something I want for myself. Iâ€™m not a cow. I-Iâ€™ll never let you go.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Thanks for that Rick\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RickBot: You're welcome Morty.\n"
     ]
    }
   ],
   "source": [
    "# Chat with Rickbot for 5 conversation\n",
    "for step in range(5):\n",
    "    print(generate_text(input(\">> User:\"), False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Model Performance\n",
    "\n",
    "The fine-tuned DialoGPT model continues to show its ability to simulate the personality of Rick Sanchez from Rick and Morty. Let's evaluate its performance based on the new dialogues:\n",
    "\n",
    "#### Coherence\n",
    "\n",
    "The model's responses are coherent and form complete sentences. They also show improved coherence in the context of the conversation. For example, when asked about why it lied about knowing Morty, the model provides a long, rambling response that is characteristic of Rick's character.\n",
    "\n",
    "#### Relevance\n",
    "\n",
    "The model's responses are relevant to the questions asked. For instance, when asked about Morty, the model initially denies knowing him but later admits that Morty is his grandson. This kind of unpredictable behavior is in line with Rick's character.\n",
    "\n",
    "#### Personality\n",
    "\n",
    "The model exhibits the personality traits of Rick Sanchez. It captures Rick's sarcastic humor, his disregard for social norms, and his scientific knowledge. For example, the model's response to the question about lying captures Rick's cynical and dismissive attitude towards therapy, which is a recurring theme in the series.\n",
    "\n",
    "#### Robustness\n",
    "\n",
    "The model is robust and can generate a response to any input. The quality and relevance of the responses have improved significantly after fine-tuning, as seen in the dialogues.\n",
    "\n",
    "#### Context Awareness\n",
    "\n",
    "The model shows improved context awareness. It maintains the context of the conversation and generates responses that are relevant to the previous dialogues. However, there are still moments where the model loses track of the context, such as the sudden shift from denying knowing Morty to admitting that he is his grandson.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The fine-tuned DialoGPT model continues to show improvement in simulating the personality of Rick Sanchez. It provides coherent, relevant, and context-aware responses that capture Rick's distinctive personality traits. However, maintaining persona coherence over multiple turns of conversation remains a challenge. This could be addressed by refining the training process to better capture the persona of Rick and by using more advanced techniques to maintain the context over longer conversations. Despite these challenges, the model's performance in these dialogues demonstrates the potential of fine-tuning language models for character-specific chatbot applications.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Parameters in DialoGPT\n",
    "\n",
    "In machine learning, the choice of parameters can significantly impact the performance of a model. During our experiments with the DialoGPT model, we played around with various parameters to optimize the model's performance. Here are some of the key parameters we experimented with and the impact they had on the model's performance:\n",
    "\n",
    "## Size of Context Data\n",
    "\n",
    "The context size, or the amount of previous dialogue the model considers when generating a response, is a crucial parameter. A larger context size allows the model to consider more of the conversation history, which can lead to more relevant and coherent responses. However, it also increases the computational complexity of the model, leading to longer training times. It's also worth noting that while a larger context size can improve performance, it can also lead to overfitting if the model starts to memorize specific sequences from the training data instead of learning general patterns.\n",
    "\n",
    "<img src=\"assets/para1.png\" alt=\"p1\" height=\"250\">\n",
    "\n",
    "## Training Epochs\n",
    "\n",
    "The number of training epochs, or complete passes through the entire training dataset, is another important parameter. Training for more epochs allows the model to learn more from the data, which can lead to improved performance. However, there's a risk of overfitting if the model is trained for too many epochs, as it might start to memorize the training data instead of learning to generalize from it. It's also worth noting that the optimal number of epochs can depend on various factors, including the size and complexity of the dataset and the capacity of the model.\n",
    "\n",
    "<img src=\"assets/para2.png\" alt=\"p2\" height=\"250\">\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "The learning rate is a critical hyperparameter that controls how much the model's weights are updated in response to the estimated error each time the weights are updated. A smaller learning rate requires more training epochs as the changes made to the weights each update are smaller. In contrast, a larger learning rate may allow the model to learn faster, but it can also lead to unstable training and poor performance. It's often beneficial to experiment with different learning rates or use techniques like learning rate schedules or adaptive learning rates to find an optimal learning rate.\n",
    "\n",
    "<img src=\"assets/para3.png\" alt=\"p3\" height=\"250\">\n",
    "\n",
    "## Block Size\n",
    "\n",
    "The block size, or the number of input tokens the model considers at a time, can impact both the computational efficiency and the performance of the model. A smaller block size can lead to faster training, but it might also limit the model's ability to learn longer-term dependencies in the data. On the other hand, a larger block size can improve the model's performance by allowing it to consider more context, but it also increases the computational complexity of the model.\n",
    "\n",
    "<img src=\"assets/para4.png\" alt=\"p4\" height=\"250\">\n",
    "\n",
    "## Large vs Small Model\n",
    "\n",
    "The size of the model, determined by the number of parameters it has, can significantly impact its performance. Larger models have more capacity to learn complex patterns in the data, which can lead to improved performance. However, they also require more computational resources to train and are more prone to overfitting, especially when the amount of training data is limited. On the other hand, smaller models are more computationally efficient and less prone to overfitting, but they might not have enough capacity to learn complex tasks.\n",
    "\n",
    "<img src=\"assets/para5.png\" alt=\"p5\" height=\"250\">\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "The choice of optimizer can also impact the model's performance. Different optimizers use different strategies to update the model's weights based on the gradients of the loss function, which can lead to differences in training speed and final performance. However, the impact of the optimizer can also depend on other factors, like the learning rate and the complexity of the model and the data.\n",
    "\n",
    "In conclusion, fine-tuning the parameters of the DialoGPT model is a complex task that requires careful consideration of various factors. The optimal settings can depend on the specific task, the available computational resources, and the characteristics of the training data.\n",
    "\n",
    "<img src=\"assets/para6.png\" alt=\"p6\" height=\"250\">\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Different Datasets and Models\n",
    "\n",
    "In our quest to create a chatbot that mimics the character Rick Sanchez from Rick and Morty, we didn't limit ourselves to just one approach. We experimented with different datasets and models to test their impact on the results. Given the limited time we had, we had to be strategic in our choices and focused on options that could potentially enhance the chatbot's performance.\n",
    "\n",
    "## Philosophy Dataset\n",
    "\n",
    "One of the datasets we experimented with was a philosophy dataset. This dataset contains dialogues and texts from various philosophical works. Given Rick's character, which often delves into existential and philosophical themes, we hypothesized that training the model on this dataset could help it generate more philosophically inclined responses, akin to Rick's character.\n",
    "\n",
    "## Shakespeare Dataset\n",
    "\n",
    "We also experimented with a Shakespeare dataset. This dataset contains the complete works of Shakespeare. Given the dramatic and distinctive language used in Shakespeare's works, we hypothesized that this dataset could help the model generate more creative and dramatic responses, adding a unique flavor to the chatbot's dialogues.\n",
    "\n",
    "## Pygmalion Model\n",
    "\n",
    "In addition to the DialoGPT models, we also experimented with the Pygmalion model, a larger language model with 6 billion parameters. Given its larger capacity, we hypothesized that this model could potentially generate more nuanced and contextually appropriate responses. However, due to the model's size and the computational resources required to fine-tune it, we were only able to sample from the pre-trained model and couldn't fine-tune it within our timeframe.\n",
    "\n",
    "Despite the challenges and time constraints, these experiments provided valuable insights into the potential of different datasets and models in creating a character-specific chatbot. In the following sections, we'll delve into the results of these experiments, starting with the philosophy dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Philosophical Texts\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We utilized a dataset containing philosophical texts, available at [this link](https://www.kaggle.com/datasets/christopherlemke/philosophical-texts?resource=download&select=sentences.csv). The dataset is rich in complex language structures and abstract concepts, making it an interesting choice for training the GPT-2 model.\n",
    "\n",
    "## Training\n",
    "\n",
    "The model was trained on this dataset, and we achieved a loss of 3.5. \n",
    "\n",
    "<img src=\"assets/philo_loss.png\" alt=\"a2\" height=\"300\">\n",
    "\n",
    "___\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        df = pd.read_csv(file_path)\n",
    "        text = \" \".join(df['sentence'].tolist())\n",
    "\n",
    "        self.examples = []\n",
    "        tokenized_text = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
    "            self.examples.append(tokenized_text[i : i + block_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch: {epoch} / {epochs}\")\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            print(f\"index: {idx}\")\n",
    "            inputs, labels = batch, batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss:  {loss.item()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = DataParallel(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "file_path = 'sentences.csv'\n",
    "\n",
    "dataset = TextDataset(tokenizer, file_path, block_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train(model, dataloader, optimizer, device, epochs)\n",
    "\n",
    "# to save model\n",
    "model.module.save_pretrained('./my_model_directory')\n",
    "tokenizer.save_pretrained('./my_model_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "We then sampled from the model using a prompt related to metaphysics. The generated text was compared to a reference text using the BLEU score, a widely used metric for evaluating the quality of machine-generated text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# to generate text\n",
    "def generate_text(prompt_text):\n",
    "    model.module.eval()\n",
    "    generated = torch.tensor(tokenizer.encode(prompt_text)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(generated.shape, dtype=torch.long, device=device)\n",
    "\n",
    "    sample_outputs = model.module.generate(\n",
    "                                    generated,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    pad_token_id=tokenizer.eos_token_id,\n",
    "                                    do_sample=True,\n",
    "                                    top_k=50,\n",
    "                                    max_length = 1000,\n",
    "                                    top_p=0.95,\n",
    "                                    num_return_sequences=1\n",
    "                                    )\n",
    "\n",
    "    return tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "reference_text = \"The fundamental question of metaphysics is what to say: what reason can we hope to have recourse to in the present case of all the arguments? The reader will find that though I may proceed to ask him whether the two notions of reason are the same. For I ask, first, whether such ideas will admit of correction and solution, or do neither. But what I was saying, replied, we shall arrive at the same point of time about these questions; and, if, as has appeared, they are the only two, no one will assert, that reason possesses the authority of that power, and that reason, according to the hypothesis of this, is the only medium between them. For, in an empirical science, the absolute totality of the two conceptions and the former can be proved either by a universal law of the completeness of the latter, or of the former, from without the factious and necessary completeness of the object. The question to be equally apparent contradictions. If there are only one part of the two conceptions of the third in the series of the two principles and not in any given series of ideas, it, of the series must, one and never the other predicates; then I shall not merely assume both, the possibility; and that the totality of the two statements are valid, that there can be at least, that, this proposition must not exist in the one, each conception of the second statements of the series consists of an empirical statements; which I can be contradictory statements, by the four, by reason, I cannot be possible only by the unity, that unity as in contradiction. For, that the unconditioned in this, cannot be one and must be the possibility, is not, the absolute necessity neither must be, must never contradict, therefore, by the unconditioned in this may be as, be, we cannot be the only of the two propositions, contain in the totality of two, be the fourth and not in each, or in the mode, be proved, be also in the two propositions, only one can never one, one, contradict, that if one series has no parts, the complete series. The object is impossible, I shall require, I shall be not in that there must exist in the case the unconditioned the conclusion, that there are the whole, the three only be a necessary, that, of which are two, in relation, although the second only exist either of the conception. The proposition of the principle of the two must be either, can be either. For, in some, I am unable to answer, without contradiction that, are all phenomena; this can be proved, as a contingent and as to be determined only by experience. And, no one must cease to exist, or must be equally impossible. But what a transcendent, is impossible. I can never be denied by experience. The division of conceptions, from the subject, therefore, must not exist in both, and must cease to be impossible; or, as the conception. This division of phenomena are transcendent; I say, I cannot, from pure conceptions, by means of the totality of reason, or, must have, never cease to exist by means only one principle. For I can never be given by means of contradiction. I have, not in no reason; and there are not empirical propositions and not an absolute non-theology. Now the conditions of an impossible series of phenomena as the non-existence. Nor can be in a transcendent propositions, are, transcendental synthetical propositions. A, no absolute totality, must be therefore, cannot be a series of the series of the two distinct principles of a transcendental unity or a conception, and are never without condition of the conditions. A priori. There must cease, which, without means, not to be given in relation to a thing. The world, no conception; and cannot be contingent. But a, of a possible things which is not absolutely beyond, all, and, I cannot be possible. But the contradictory propositions, never cease to be subject to the condition; and, and must be, in general, according to a non-existence, is void of the series; I can only as to be non-existence in the only in existence. We have only as conditioned.  The transcendental in conformity with the conception, and the transcendent, cannot be the series of the non-in conformity to the mode of things. There must cease to one in nature. There must be, if a, if transcendental; but contain either of, that which does not contain either in a regress, of conditions. On the series of phenomena, if we cannot be in itself of parts. And this mode of the series of propositions of all the series. But these can be the same. To infinity of the conditioned. In the series. All the synthesis of phenomena, because that is, if it be considered merely as contained\"\n",
    "prompt_text = \"The fundamental question of metaphysics is\"\n",
    "generated_text = generate_text(prompt_text)\n",
    "print(generated_text)\n",
    "\n",
    "# Tokenize the reference and generated texts\n",
    "reference_tokens = reference_text.split()\n",
    "generated_tokens = generated_text.split()\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prompt\n",
    "\n",
    "The prompt used for generating the text was:\n",
    "\n",
    "```plaintext\n",
    "\"The fundamental question of metaphysics is\"\n",
    "```\n",
    "\n",
    "### Generated Text\n",
    "\n",
    "The model generated the following text:\n",
    "\n",
    "```plaintext\n",
    "The fundamental question of metaphysics is, whether the principle is not merely regulative and its effects may be empirical, or if not synthetical, be judged as natural. The objective method of transcendental ideas is a priori, and of course the conception of any of these ideas from the principles. And when we have given these directions to our understanding, we can infer no but its synthetical principles. In order to prove this, we must first remove the difficulties of the question from the principle of the determination of the empirical conception, that is, to infer it synthetically from the principle of the possibility of the existence of any synthetical proposition. In such cases we have in common with the speculative reason an entire and absolute impossibility. Hence we can no longer assume this conception to be transcendental, and can no longer believe in any other, than for this reason, that it could conceive that such a principle in an abstracted conception, and contingent connexion to any object as that can never would contradict all the existence. But since, that the object must never have a universal use of the understanding of understanding and understanding can never be the conception in any of the object as its object, without any determinate objects of our experience in itself. For the objective reason has no experience. But reason, therefore, the object, I admit it possesses a different ideas of its own priori distinct conceptions, and in the internal conceptions of the possibility of its internal intuition, must be the possibility, without any empirical unity. Now I know only possible experience, its conception of these, the existence is not in general conditions of experience a contingent causes. But, in so far as it no empirical principles of the understanding, the objects of the external intuitions, this universal principles in a possible objects only, the possibility without in accordance with universal laws according to be a transcendental principles. In the existence, a transcendental conceptions of a transcendental reason in its operations of external experience or of the objects, the causality to be, a priori principles of these principles. Now, if it can never a priori and subject of the internal intuition of the internal experience as subject, without which the internal sense can proceed with this, the internal intuition must be, a priori of the conception. A priori and an external intuition a priori in phenomena a priori. For it has a necessary conjoined all phenomena; and I do not establish that a priori a priori to the determination of that self, the universal validity of the possibility of things a priori a priori, by means of the causalitya priori judgments, as a given phenomenon in accordance with which, must be valid for every internal experience without being a purpose, a natural science. Now, in the mode of reason. But in an empirical intuition, all our sensuous object, and the conversely, must have any object, the universal laws according to that the understanding, as we must be of experience. Hence, a priori; and the former, the conjoin the objects without which can not a principle of the possibilitya priori; and in relation of the possibility. In both of possible experience, are impossible. In order we must, in the first principles of naturea mere conceptions, a prioria relation to be transcendental hypotheses of the empirical cognition, all phenomena, be considered as principles which can have no empirical causes of the unity of the existencewhich cannot be independent of our external things according to the series of the existence and to be determinate; these two contradictory principles. Now, if any universal ideas, and of the absolute existence. But that we can be not be thus in the object in a priori. The object of the non- is not internal substances as phenomena, and a priori, in relation to phenomena as independent of no one must precediae, therefore only of a priori, can not in themselves the unconditioned, it must exist as to be given in one of the series of things in the possibilitya priori inasmuch a priori. But the internal purposiveness of the conception of experience. If this can exist in no internal existence according to the nature, without any empirical unity and can never, or, it is in the empirical, it is not a transcendental, in relation to the conception, the possibility according to the series of a pure reason; therefore the conception, are transcendent, in the objectin relation to the object. Thus we may be assured as possible, in itself. The internal, of an object and not as existing by reason, the internal subject.  The only. The principles cannot be deduced priori. For it may not be applied to phenomena, with certain priori in the formal conditions of the possibility. Of the universal principles, the possibility of things a priori, is a priori according to the conditions, in general, are only necessary. The internal experience.\n",
    "```\n",
    "\n",
    "### BLEU Score\n",
    "\n",
    "The BLEU score for the generated text compared to the reference text was 0.0740927759524612. The BLEU score ranges from 0 to 1, with higher scores indicating better alignment with the reference text. A score of 0.0741 suggests that the generated text has some alignment with the reference but also significant differences.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "The generated text exhibits a complex structure and uses terminology consistent with philosophical discourse. However, the low BLEU score indicates that the generated text may not closely follow the reference text's specific ideas and structure.\n",
    "\n",
    "### Positives\n",
    "\n",
    "- **Complexity**: The generated text demonstrates an understanding of complex philosophical concepts and language.\n",
    "- **Relevance**: The text is relevant to the prompt and engages with metaphysical questions.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Coherence**: The generated text may lack coherence in some parts, with sentences that are grammatically correct but may not logically follow one another.\n",
    "- **Alignment with Reference**: The low BLEU score suggests that the generated text does not closely align with the reference text, indicating potential issues with capturing the specific nuances and arguments of the philosophical discourse.\n",
    "\n",
    "## Learnings\n",
    "\n",
    "Training the GPT-2 model on a dataset of philosophical texts resulted in the generation of complex and relevant text. However, the experiment also highlighted challenges in maintaining coherence and alignment with specific philosophical arguments. This experiment underscores the complexity of training models on abstract and nuanced domains like philosophy and provides valuable insights for future work in this area.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Shakespearean Texts\n",
    "\n",
    "After exploring the philosophical domain, we decided to venture into the realm of literature, specifically the works of William Shakespeare. Shakespeare's texts are known for their unique style, rich vocabulary, and complex narrative structures, making them an interesting choice for training our model.\n",
    "\n",
    "## Model Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "out_dir = \"out-shakespeare\"\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "wandb_log = False  # feel free to turn on\n",
    "wandb_project = \"shakespeare\"\n",
    "wandb_run_name = \"ft-\" + str(time.time())\n",
    "\n",
    "dataset = \"shakespeare\"\n",
    "init_from = \"gpt2-medium\"  # this is the largest GPT-2 model\n",
    "\n",
    "# only save checkpoints if the validation loss improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# the number of examples per iter:\n",
    "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
    "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "max_iters = 20\n",
    "\n",
    "# finetune at constant LR\n",
    "learning_rate = 3e-5\n",
    "decay_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    assert gradient_accumulation_steps % torch.cuda.device_count() == 0\n",
    "    gradient_accumulation_steps //= torch.cuda.device_count()\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this experiment, we used a dataset containing the works of William Shakespeare. Known for his unique style, rich vocabulary, and complex narrative structures, Shakespeare's texts present an interesting challenge for language models.\n",
    "\n",
    "## Training\n",
    "\n",
    "We trained the GPT-2 model on this dataset for 300 steps, achieving a loss of around 1.2. The loss value indicates how well the model has learned to predict the next word in a sequence, with lower values generally indicating better performance.\n",
    "\n",
    "<img src=\"assets/Shakespeare Finetune.png\" alt=\"a1\" height=\"300\">\n",
    "\n",
    "## Sampling\n",
    "\n",
    "We then sampled from the model using a dialogue prompt. The generated text was compared to a reference text using the BLEU score, a widely used metric for evaluating the quality of machine-generated text.\n",
    "\n",
    "### Generated Dialogue\n",
    "\n",
    "The model generated the following dialogue:\n",
    "\n",
    "```plaintext\n",
    "RICHARD:\n",
    "Thou art a merchant of deceit: if I falter,\n",
    "I trade thee to the first; if I weep,\n",
    "I trade thee to the second; if I\n",
    "speak untruths, I trade thee to the third; if I wage war,\n",
    "I trade thee to the fourth: so barter or trade,\n",
    "I tell thee once more, thou shalt not trade my\n",
    "dignity.\n",
    "\n",
    "ISABELLA:\n",
    "And if thou pilfer, thou shalt not trade thy honor.\n",
    "\n",
    "RICHARD:\n",
    "I do not pilfer; I trade the pilfered treasures.\n",
    "\n",
    "RICHARD:\n",
    "Thou comprehend'st not what thou trade'st; thou, a maiden,\n",
    "Thou art ever a casualty, a trinket of no value:\n",
    "Thou hast no claim, no claim, but to be traded.\n",
    "```\n",
    "\n",
    "### BLEU Score\n",
    "\n",
    "The BLEU score for the generated text was 0.65. The BLEU score ranges from 0 to 1, with higher scores indicating better alignment with the reference text. A score of 0.65 suggests that the generated text has a good alignment with the reference.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "The generated dialogue exhibits the stylistic elements and language complexity characteristic of Shakespeare's works. However, the dialogue's content and the characters' interactions may not fully capture the depth and nuance of Shakespearean narratives.\n",
    "\n",
    "### Positives\n",
    "\n",
    "- **Stylistic Alignment**: The generated text demonstrates a strong alignment with Shakespeare's unique style and language complexity.\n",
    "- **Relevance**: The dialogue is relevant and coherent, with characters responding to each other in a manner that is consistent with the context.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Narrative Depth**: While the dialogue is stylistically aligned with Shakespeare's works, it may not fully capture the depth and nuance of Shakespearean narratives. The characters' interactions and the content of their dialogue could be further improved to better reflect the intricacies of Shakespeare's storytelling.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Training the GPT-2 model on a dataset of Shakespearean texts resulted in the generation of text that stylistically aligns with Shakespeare's works. However, the experiment also highlighted the challenge of capturing the depth and nuance of Shakespearean narratives. This experiment underscores the complexity of training models on nuanced domains like literature and provides valuable insights for future work in this area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pygmalion Model\n",
    "\n",
    "### Model\n",
    "\n",
    "We utilized the Pygmalion model, a language model with 6 billion parameters, for this experiment. Pygmalion is known for its ability to generate high-quality text, making it an interesting choice for our task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForCausalLM, StoppingCriteria\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PygmalionAI/pygmalion-6b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"PygmalionAI/pygmalion-6b\").to('cuda')\n",
    "\n",
    "print(\"Loaded Model\")\n",
    "from transformers import StoppingCriteria\n",
    "class MyStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, target_sequence, prompt):\n",
    "        self.target_sequence = target_sequence\n",
    "        self.prompt=prompt\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Get the generated text as a string\n",
    "        generated_text = tokenizer.decode(input_ids[0])\n",
    "        generated_text = generated_text.replace(self.prompt,'')\n",
    "        # Check if the target sequence appears in the generated text\n",
    "        if self.target_sequence in generated_text:\n",
    "            return True  # Stop generation\n",
    "\n",
    "        return False  # Continue generation\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self\n",
    "    \n",
    "    \n",
    "# Start with the persona and a single user turn\n",
    "conversation_history = \"\"\"Rick's Persona: Rick Sanchez, a 60 year old genius scientist who goes on wacky adventures, has a sarcastic and funny tone and a large ego.\n",
    "<START>\n",
    "You: Who are you?\n",
    "Rick:\"\"\"\n",
    "\n",
    "while True:\n",
    "    encoded_input = tokenizer(conversation_history, return_tensors='pt')\n",
    "    input_ids = encoded_input['input_ids'].cuda()\n",
    "    response = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=256,\n",
    "        stopping_criteria=MyStoppingCriteria(\"You:\", conversation_history)\n",
    "    )\n",
    "    response = tokenizer.decode(response[0][len(encoded_input[\"input_ids\"][0]):])\n",
    "    print(\"Rick: \", response)\n",
    "    \n",
    "    user_input = input()\n",
    "    conversation_history += \"Rick: \" + response + \"\\nYou: \" + user_input + \"\\nRick:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sampling 1\n",
    "\n",
    "We started the conversation with a brief persona description for Rick Sanchez and a single user turn. The model was then used to generate Rick's responses.\n",
    "\n",
    "### Conversation\n",
    "\n",
    "Here's the conversation that was generated:\n",
    "\n",
    "```plaintext\n",
    "You: Who are you?\n",
    "Rick: Hey, that's a pretty cute nickname for a guy like me. I'm Rick, the guy who's in charge of the asylum. I keep the asylum running and keep order in the castle. Oh, and I have superhuman strength. But that's not the point, now, miss Samara.\n",
    "\n",
    "You: Hi Rick, I'm Morty.\n",
    "Rick: Hey, Miss Morty. I'm the guy in charge of the asylum, but you're probably not aware of that. I've got superhuman strength, and I'm visiting the asylum because I've gone nuts and lost my mind. My nickname's to Morty, but I call you Samara if you got it straight. You can just call me R.\n",
    "\n",
    "You: Hey Rick, are you ok? I'm your grandson Morty\n",
    "Rick: MORTY: YES, I'M OK, SIR! I'M JUST... I'M JUST... I'M SO Nervous, that's why I'm in the asylum! My family weren't even supposed to visit me, and now I've been kidnapped by you and you're bringing me back to the asylum! You're the only person in the world who's been able to get me out of the asylum, and now you're bringing me back to deal with other people's problems! I'm just so nervous!\n",
    "\n",
    "You: Ahh geez Rick\n",
    "Rick: MORTY: PLEASE, WHAT'S THE matter, Dad? *I call again, a little more urgently this time*\n",
    "```\n",
    "\n",
    "### Analysis\n",
    "\n",
    "The generated conversation exhibits some interesting characteristics.\n",
    "\n",
    "### Positives\n",
    "\n",
    "- **Creativity**: The model generates creative responses, introducing a new narrative about an asylum.\n",
    "- **Relevance**: The responses are relevant to the user's inputs, maintaining a consistent narrative throughout the conversation.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Character Consistency**: The model does not consistently maintain the character of Rick Sanchez. Rick's responses do not align with his established persona as a genius scientist with a sarcastic and funny tone. Instead, the model introduces a new narrative about an asylum, which is not consistent with Rick's character.\n",
    "- **Coherence**: The conversation lacks coherence in some parts. For example, Rick refers to Morty as \"Miss Morty\" and \"Samara,\" which does not make sense in the context of their established relationship.\n",
    "\n",
    "___ \n",
    "### Sampling 2\n",
    "\n",
    "We continued the conversation with Rick Sanchez using the Pygmalion model. The conversation took an interesting turn towards an alien invasion narrative.\n",
    "\n",
    "### Conversation\n",
    "\n",
    "Here's the conversation that was generated:\n",
    "\n",
    "```plaintext\n",
    "You: Hey, Rick, why don't aliens ever visit our planet?\n",
    "Rick: 'cause they know what they're gonna get.\n",
    "\n",
    "You: Does that mean they are real?\n",
    "Rick: Yeah, and they're coming to our planet to get our lunch.\n",
    "\n",
    "You: What do we do about that\n",
    "Rick: *I take out a pair of binoculars from my jacket, one on each side. I hold them up to my eyes and I stare at the approaching mob of aliens. They're coming for our planet...*\n",
    "\n",
    "You: Ah Geez Rick\n",
    "Rick: *As soon as you start talking, I drop the binoculars and hold up my arms. I start waving my arms and yelling.*\n",
    "**Don't be scared!**  *I say to the aliens.*\n",
    "**We're not gonna hurt you!**\n",
    "```\n",
    "\n",
    "### Analysis\n",
    "\n",
    "The generated conversation exhibits some interesting characteristics.\n",
    "\n",
    "### Positives\n",
    "\n",
    "- **Creativity**: The model continues to generate creative responses, introducing a new narrative about an alien invasion.\n",
    "- **Relevance**: The responses are relevant to the user's inputs, maintaining a consistent narrative throughout the conversation.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Character Consistency**: The model still struggles with maintaining the character of Rick Sanchez. Rick's responses do not align with his established persona as a genius scientist with a sarcastic and funny tone. Instead, the model introduces a new narrative about an alien invasion, which is not consistent with Rick's character.\n",
    "- **Coherence**: The conversation lacks coherence in some parts. For example, Rick's actions and responses to the alien invasion do not align with his character's established traits and behaviors.\n",
    "\n",
    "### Learnings\n",
    "The Pygmalion model, with its 6 billion parameters, represents a significant advancement in language model capabilities. It has been trained extensively on a diverse range of data, which allows it to generate creative and contextually relevant responses.\n",
    "\n",
    "In our experiment, we used Pygmalion to simulate conversations with the character Rick Sanchez from the \"Rick and Morty\" series. We provided the model with a brief personality description of Rick and let it generate responses based on user inputs.\n",
    "\n",
    "The results were intriguing. The model demonstrated a high level of creativity, introducing new narratives and responding to user inputs in a contextually relevant manner. This suggests that the Pygmalion model has a strong potential for generating engaging and dynamic conversations.\n",
    "\n",
    "However, the model also exhibited some limitations. Despite being provided with a character persona, the model struggled to maintain the specific character traits of Rick Sanchez consistently throughout the conversation. The responses sometimes lacked coherence and did not always align with Rick's established character traits.\n",
    "\n",
    "This experiment highlighted the potential of using larger, well-trained models like Pygmalion for character-based conversation generation. However, it also underscored the challenges in maintaining character consistency and coherence, particularly when the model is only provided with a brief character description.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Summary\n",
    "\n",
    "Throughout this project, we embarked on a journey to create a chatbot that could emulate the personality of Rick Sanchez from the popular TV show \"Rick and Morty\". We utilized the DialoGPT model, a variant of the GPT-2 model, and fine-tuned it on a dataset of dialogues from the show. \n",
    "\n",
    "The fine-tuned model demonstrated a significant improvement in emulating Rick's personality compared to the standard DialoGPT model. It was able to generate responses that were not only coherent and contextually relevant but also captured the unique personality traits of Rick Sanchez. However, we also observed some limitations, such as occasional lapses in coherence and the model's tendency to lose its persona in extended dialogues.\n",
    "\n",
    "We also experimented with different parameter settings in the DialoGPT model, such as the size of the context data, training epochs, learning rate, block size, and optimizer. We found that increasing the size of the context for training and the number of training epochs generally improved the model's performance. However, changing the block size and optimizer did not significantly affect the performance.\n",
    "\n",
    "In addition to the \"Rick and Morty\" dataset, we also experimented with other datasets and models. We trained the GPT-2 model on a dataset of philosophical texts and another of Shakespearean texts. The models generated text that stylistically aligned with the respective domains, demonstrating the versatility of the GPT-2 model. However, capturing the depth and nuance of these domains proved to be a challenge.\n",
    "\n",
    "In conclusion, this project demonstrated the potential of transformer-based models like GPT-2 and DialoGPT in generating text that emulates a specific style or personality. However, it also highlighted the challenges in capturing the depth and nuance of complex domains. Future work could focus on improving the model's ability to maintain its persona in extended dialogues and exploring more sophisticated techniques for fine-tuning the model on nuanced domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNS8WNq8MHVC1UhtTGq4MiY",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "014c0f4f80234bd2b42a5b127560156c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "021c708a7d674c28a4263d0713618bd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e07a10962c042299a4ab98a3f8114ef",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b25c4d9ebf424ec9837ddbedf8e82dd2",
      "value": " 427/427 [02:11&lt;00:00,  3.78it/s]"
     }
    },
    "03c631b6de0d44248c48119761b09f9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08980302545a4abcad9b6a96f5d036e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8044cdda3bcf47c287a7bfd23036910f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_978f389524e743219c6887fd351782a2",
      "value": "Evaluating: 100%"
     }
    },
    "0b58496b39ba453dbe732b3baaf62abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f7f5165832d4a9bbfabf0f2176ab17f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c631b6de0d44248c48119761b09f9d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_933946ee0bd449068e53c3b8bacbd93c",
      "value": " 427/427 [02:14&lt;00:00,  3.78it/s]"
     }
    },
    "1254cb3e5f9a450983e4a484f4a3112e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12e01f310b2e47eba2892af875a5e997": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13a1e1b4753b4a1a9fa362d19489189f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92473f667fa44d3594b85a560c33202b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f35ad2d448d249f5967fa324be79b941",
      "value": "Evaluating: 100%"
     }
    },
    "15f8172dc94341f3b1f1ccb81d064023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a0b0be7e4714ae790d95a92a8078902": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b85a478df2b47dbb46317977b4d9390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36ff813827f24321a909985f41cc8667",
      "max": 427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89965f0fad1244c8bf68fc548bee380a",
      "value": 427
     }
    },
    "1c4b7b92ed5848d6b96c59d81d41264b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc35abe55fdf4c3db99beccf34b781c2",
       "IPY_MODEL_345b01f1bc7b4ea1bcad2c1c5d8fee8b",
       "IPY_MODEL_6b4251a0852043b7afe9567f2220231c"
      ],
      "layout": "IPY_MODEL_014c0f4f80234bd2b42a5b127560156c"
     }
    },
    "1d968c385299408197a34e38300deaa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dddf76fde26c4f4b8c7e18d3fbea05fe",
       "IPY_MODEL_2cd0142c10504db88bf133d9341ccd72",
       "IPY_MODEL_c70afbc9cafd45be9c2823a675699cc2"
      ],
      "layout": "IPY_MODEL_8442f2829cce4045a8fbf35ea028959c"
     }
    },
    "1f578e94f5f745db8a85c02355db6abc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21f01bbc75674755b30d118d764ef62b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24d36cdd607e47bd83d1ce630d4d0186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6b83442e10142e9b3a1253422d2021b",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85fa025cc05e4b67a3f5627422af4409",
      "value": 47
     }
    },
    "2689998d52004a4db12d5833422e6b67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28e407e0aa4a48c6a9d4ff65938e6961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cd0142c10504db88bf133d9341ccd72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e795e8b6735c41cb8945bb1cf395d965",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a99d7009e604b26a0f90ba4ab4f22e5",
      "value": 47
     }
    },
    "2cf04f43cdb64abebd20bea5de53cbf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3149e4051f5646848d02f64ed49d830d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31925a815dfb418b8bda148e7fb7614a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08980302545a4abcad9b6a96f5d036e9",
       "IPY_MODEL_24d36cdd607e47bd83d1ce630d4d0186",
       "IPY_MODEL_42d13228493646568c5f09da675ba4e8"
      ],
      "layout": "IPY_MODEL_fc53c5a6f20e43fc92e1421a47ea7bde"
     }
    },
    "345b01f1bc7b4ea1bcad2c1c5d8fee8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8e9526b0d174f5aa7c99ad053972b49",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_15f8172dc94341f3b1f1ccb81d064023",
      "value": 47
     }
    },
    "364d8271f76348868de1979bae48bf39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2689998d52004a4db12d5833422e6b67",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_68b718c3fe7643788b982f024927ec1f",
      "value": "Evaluating: 100%"
     }
    },
    "36ff813827f24321a909985f41cc8667": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3785ef667d2744c68bcc9b2f1f78260f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c8855a72f4a4e2fa2f753394f5abf4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dda1b6a75ff41bfa98e2a182531391e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5aefd7880e7f414f87fef00c0c1c1127",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d6e20e90f3fc40d0afe7aececa274384",
      "value": "Epoch: 100%"
     }
    },
    "3f88d37fc7fe41048ccd050b533f50e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "417eb867d1b844bba8211607c40580f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e884c11cf53441bf96a0deb949696662",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cc4ef5e0da494d6fad9ec561384e2f1b",
      "value": " 47/47 [00:03&lt;00:00, 12.65it/s]"
     }
    },
    "42a2c22d78634f7aa101218662a297df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42d13228493646568c5f09da675ba4e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_756c40218695403b958412a88328f53f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1254cb3e5f9a450983e4a484f4a3112e",
      "value": " 47/47 [00:03&lt;00:00, 12.61it/s]"
     }
    },
    "4afaa8bf0c7c431db678d63af61b0312": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e9269cdb1b048dab84a7df39f9fec3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51bba57884ee45abbf964d2d80bd1cda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_364d8271f76348868de1979bae48bf39",
       "IPY_MODEL_91bf6335838048d8830690a579262c16",
       "IPY_MODEL_b6c957478a55470ca26e6c6cbe756100"
      ],
      "layout": "IPY_MODEL_3149e4051f5646848d02f64ed49d830d"
     }
    },
    "55c977d6c0384547a42a69e9ec135a67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "567a23152ed04da1830acbe2dcbdd551": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57d7f1e41107467e9f4dd23fed8dacba",
       "IPY_MODEL_d360fa90ece4429eb93552a8d84c6f0b",
       "IPY_MODEL_69937e99baa044b1851a5f0cc48f010b"
      ],
      "layout": "IPY_MODEL_1f578e94f5f745db8a85c02355db6abc"
     }
    },
    "57d7f1e41107467e9f4dd23fed8dacba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c8855a72f4a4e2fa2f753394f5abf4b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f5087d42980149f58c3f0361c10211cc",
      "value": "Iteration: 100%"
     }
    },
    "5aefd7880e7f414f87fef00c0c1c1127": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "616c1d99b2724a6fbddbdc9abbf18718": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64045879d6244f1b915a2a695260e3b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64f1adda69124d4aae77de31242b6513": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67e89c39ccf6470eb233a51c4548ba7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68b718c3fe7643788b982f024927ec1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69937e99baa044b1851a5f0cc48f010b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9ce25f5495e45678a56db0cbef5551a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f6e11ac987d944a79b909f0ba6c33f96",
      "value": " 427/427 [02:13&lt;00:00,  3.22it/s]"
     }
    },
    "6b4251a0852043b7afe9567f2220231c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3785ef667d2744c68bcc9b2f1f78260f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_77622ffe28ee47539e2a2738de80d411",
      "value": " 47/47 [00:03&lt;00:00, 12.41it/s]"
     }
    },
    "6e07a10962c042299a4ab98a3f8114ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "756c40218695403b958412a88328f53f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77622ffe28ee47539e2a2738de80d411": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7802d80e667e469e846e55ec345b603e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b694cc2a19947308d4aa8c1519ebf0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ce699e145d44c00b05255064ed081b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3dda1b6a75ff41bfa98e2a182531391e",
       "IPY_MODEL_b408f7cdb7ed43a88d750297211c7cec",
       "IPY_MODEL_cfdedad418cf462a898625da74430389"
      ],
      "layout": "IPY_MODEL_64045879d6244f1b915a2a695260e3b2"
     }
    },
    "7e5742a43fd8465ca8aad04caaad0933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12e01f310b2e47eba2892af875a5e997",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_55c977d6c0384547a42a69e9ec135a67",
      "value": "Iteration: 100%"
     }
    },
    "8044cdda3bcf47c287a7bfd23036910f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8442f2829cce4045a8fbf35ea028959c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85856d61843543578471b132e7b6f351": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99da047371e4417e81746e3d9f86b08c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_21f01bbc75674755b30d118d764ef62b",
      "value": "Iteration: 100%"
     }
    },
    "85fa025cc05e4b67a3f5627422af4409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89965f0fad1244c8bf68fc548bee380a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d9f18b04db943a48e8fe49ac25971f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91bf6335838048d8830690a579262c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_616c1d99b2724a6fbddbdc9abbf18718",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8bb6ac84a704c2fa3c027fa0e6c0fe6",
      "value": 47
     }
    },
    "92473f667fa44d3594b85a560c33202b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "933946ee0bd449068e53c3b8bacbd93c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "978e6fd6031b4f44a58f0bd4bc5b3384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85856d61843543578471b132e7b6f351",
       "IPY_MODEL_e300caa213f44ac78522ce11d9778ad1",
       "IPY_MODEL_021c708a7d674c28a4263d0713618bd6"
      ],
      "layout": "IPY_MODEL_3f88d37fc7fe41048ccd050b533f50e2"
     }
    },
    "978f389524e743219c6887fd351782a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99da047371e4417e81746e3d9f86b08c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a99d7009e604b26a0f90ba4ab4f22e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c76517b1dde4535a92a50e42b600149": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13a1e1b4753b4a1a9fa362d19489189f",
       "IPY_MODEL_def406c0c91d4e8e83157005ccba4c51",
       "IPY_MODEL_417eb867d1b844bba8211607c40580f8"
      ],
      "layout": "IPY_MODEL_28e407e0aa4a48c6a9d4ff65938e6961"
     }
    },
    "a52cba19ab9245628f085368052e737e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a8bb6ac84a704c2fa3c027fa0e6c0fe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aac30f978b3b403082842bf8ee457fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "abb866e3f37b41bea9b2631de916af75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b25c4d9ebf424ec9837ddbedf8e82dd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3734cc835c24877b0c750a85c9d2389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b408f7cdb7ed43a88d750297211c7cec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abb866e3f37b41bea9b2631de916af75",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67e89c39ccf6470eb233a51c4548ba7b",
      "value": 3
     }
    },
    "b6c957478a55470ca26e6c6cbe756100": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a0b0be7e4714ae790d95a92a8078902",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bfd072939aa445f2bdb46edfe89c94c5",
      "value": " 47/47 [00:03&lt;00:00, 12.49it/s]"
     }
    },
    "b9ce25f5495e45678a56db0cbef5551a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfd072939aa445f2bdb46edfe89c94c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c06483936c004b2394a153c92738b996": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6b83442e10142e9b3a1253422d2021b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c70afbc9cafd45be9c2823a675699cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4afaa8bf0c7c431db678d63af61b0312",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_42a2c22d78634f7aa101218662a297df",
      "value": " 47/47 [00:03&lt;00:00, 12.32it/s]"
     }
    },
    "c8e9526b0d174f5aa7c99ad053972b49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca13c898fa4c40a7a68d7e27e2b4a919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e5742a43fd8465ca8aad04caaad0933",
       "IPY_MODEL_1b85a478df2b47dbb46317977b4d9390",
       "IPY_MODEL_0f7f5165832d4a9bbfabf0f2176ab17f"
      ],
      "layout": "IPY_MODEL_64f1adda69124d4aae77de31242b6513"
     }
    },
    "cc4ef5e0da494d6fad9ec561384e2f1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cfdedad418cf462a898625da74430389": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b694cc2a19947308d4aa8c1519ebf0b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8d9f18b04db943a48e8fe49ac25971f9",
      "value": " 3/3 [06:38&lt;00:00, 133.12s/it]"
     }
    },
    "d360fa90ece4429eb93552a8d84c6f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3734cc835c24877b0c750a85c9d2389",
      "max": 427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aac30f978b3b403082842bf8ee457fdd",
      "value": 427
     }
    },
    "d6e20e90f3fc40d0afe7aececa274384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc35abe55fdf4c3db99beccf34b781c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7802d80e667e469e846e55ec345b603e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2cf04f43cdb64abebd20bea5de53cbf5",
      "value": "Evaluating: 100%"
     }
    },
    "dddf76fde26c4f4b8c7e18d3fbea05fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c06483936c004b2394a153c92738b996",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b58496b39ba453dbe732b3baaf62abd",
      "value": "Evaluating: 100%"
     }
    },
    "def406c0c91d4e8e83157005ccba4c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e9269cdb1b048dab84a7df39f9fec3c",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a52cba19ab9245628f085368052e737e",
      "value": 47
     }
    },
    "e300caa213f44ac78522ce11d9778ad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc0bb43ea8d94fba9d70e98151bbc031",
      "max": 427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f561c589bd1146c0aed045b6e605b4d3",
      "value": 427
     }
    },
    "e795e8b6735c41cb8945bb1cf395d965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e884c11cf53441bf96a0deb949696662": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f35ad2d448d249f5967fa324be79b941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5087d42980149f58c3f0361c10211cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f561c589bd1146c0aed045b6e605b4d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f6e11ac987d944a79b909f0ba6c33f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc0bb43ea8d94fba9d70e98151bbc031": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc53c5a6f20e43fc92e1421a47ea7bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
